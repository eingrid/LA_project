{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23254/1518327901.py:6: DtypeWarning: Columns (0,1,6,7,8,10,17,18,20,21,25) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('./data/processed/ukrainian_tweets_processed.csv')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "\n",
    "#change dataset path\n",
    "df = pd.read_csv('./data/processed/ukrainian_tweets_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df['text_processed'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_documents = df['text_processed'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install cleantext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# from cleantext import clean\n",
    "\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# # Function to preprocess documents\n",
    "# def preprocess(document):\n",
    "#     # Tokenize\n",
    "#     words = word_tokenize(document.lower())\n",
    "#     # Remove stopwords and punctuations\n",
    "#     filtered_words = [word for word in words if word.isalnum() and not word in stop_words]\n",
    "#     # Lemmatize\n",
    "#     lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
    "#     return ' '.join(lemmatized_words)\n",
    "\n",
    "# # Preprocess all documents\n",
    "# preprocessed_documents = [preprocess(doc) for doc in documents]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim import matutils\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.9, min_df=4)\n",
    "tfidf = tfidf_vectorizer.fit_transform(preprocessed_documents)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NMF(n_components=10, init='random', random_state=0)\n",
    "W = model.fit_transform(tfidf)\n",
    "H = model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the top words for each topic\n",
    "n_top_words = 10\n",
    "topics = []\n",
    "for topic_idx, topic in enumerate(H):\n",
    "    top_features_ind = topic.argsort()[:-n_top_words - 1:-1]\n",
    "    top_features = [tfidf_feature_names[i] for i in top_features_ind]\n",
    "    topics.append(top_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert the list of top words into a list of lists of words\n",
    "# texts = [[word for word in doc.lower().split() if word in tfidf_feature_names] for doc in preprocessed_documents]\n",
    "\n",
    "# # Create a Gensim dictionary\n",
    "# dictionary = Dictionary(texts)\n",
    "\n",
    "# # Convert the dictionary and the corpus\n",
    "# corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# # Calculate the coherence score using Gensim\n",
    "# coherence_model = CoherenceModel(topics=topics, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "# coherence_score = coherence_model.get_coherence()\n",
    "\n",
    "# print('Coherence Score:', coherence_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# Convert tfidf_feature_names to a set for faster lookup\n",
    "tfidf_feature_names_set = set(tfidf_feature_names)\n",
    "\n",
    "def preprocess_document(doc):\n",
    "    # Lower and split the document only once, and filter using the set\n",
    "    return [word for word in doc.lower().split() if word in tfidf_feature_names_set]\n",
    "\n",
    "# Use parallel processing to optimize the conversion of documents into lists of words\n",
    "with Pool() as pool:\n",
    "    texts = pool.map(preprocess_document, preprocessed_documents)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Score: 0.5320522264948864\n"
     ]
    }
   ],
   "source": [
    "# Create a Gensim dictionary\n",
    "dictionary = Dictionary(texts)\n",
    "\n",
    "# Convert the dictionary and the corpus\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# Calculate the coherence score using Gensim\n",
    "coherence_model = CoherenceModel(topics=topics, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "coherence_score = coherence_model.get_coherence()\n",
    "\n",
    "print('Coherence Score:', coherence_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['україна',\n",
       "  'зсу',\n",
       "  'росія',\n",
       "  'украин',\n",
       "  'россиїй',\n",
       "  'войнасукраиной',\n",
       "  'агрессияроссии',\n",
       "  'вторжениероссии',\n",
       "  'війна',\n",
       "  'війнапутін'],\n",
       " ['слава',\n",
       "  'україна',\n",
       "  'герой',\n",
       "  'воїн',\n",
       "  'перемогти',\n",
       "  'зсу',\n",
       "  'смерть',\n",
       "  'славаукраїна',\n",
       "  'гера',\n",
       "  'ворог'],\n",
       " ['ебати',\n",
       "  'іти',\n",
       "  'путін',\n",
       "  'нахувати',\n",
       "  'путина',\n",
       "  'росія',\n",
       "  'хуйло',\n",
       "  'росіянин',\n",
       "  'війнути',\n",
       "  'померти'],\n",
       " ['так',\n",
       "  'робити',\n",
       "  'але',\n",
       "  'виглядати',\n",
       "  'зробити',\n",
       "  'єс',\n",
       "  'працювати',\n",
       "  'жити',\n",
       "  'казати',\n",
       "  'питання'],\n",
       " ['анімація',\n",
       "  'отже',\n",
       "  'війна',\n",
       "  'украин',\n",
       "  'война',\n",
       "  'орієнтовно',\n",
       "  'противник',\n",
       "  'скласти',\n",
       "  'течія',\n",
       "  'врятований'],\n",
       " ['дякувати',\n",
       "  'паня',\n",
       "  'підтримка',\n",
       "  'ви',\n",
       "  'зсу',\n",
       "  'пані',\n",
       "  'приємно',\n",
       "  'захисник',\n",
       "  'навзаєм',\n",
       "  'великий'],\n",
       " ['__', 'то', 'ви', 'бути', 'та', 'але', 'пані', 'мен', 'ні', 'ти'],\n",
       " ['день', 'шо', 'знати', 'та', 'не', 'хотіти', 'робити', 'як', 'мати', 'ви'],\n",
       " ['це',\n",
       "  'точно',\n",
       "  'правда',\n",
       "  'скарб',\n",
       "  'життя',\n",
       "  'місто',\n",
       "  'жах',\n",
       "  'зробити',\n",
       "  'смішно',\n",
       "  'бачити'],\n",
       " ['ну',\n",
       "  'але',\n",
       "  'звісно',\n",
       "  'казати',\n",
       "  'тип',\n",
       "  'точно',\n",
       "  'писати',\n",
       "  'взагалі',\n",
       "  '___',\n",
       "  'зрозуміти']]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "extractedts\n",
       "2022-08-19    17663\n",
       "2022-08-20     5516\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['extractedts'].apply(lambda x : x.split(' ')[0]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
