{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_185433/1952048993.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "from src.nnmf_pipeline import NNMFPipelineEnglish\n",
    "from src.lsa_pipeline import LSAPipelineEnglish\n",
    "#change dataset path\n",
    "df = pd.read_csv('data/processed/en_tweets_processed.csv')\n",
    "df = df[df.language == 'en']\n",
    "df = df[~df['text'].isna()]\n",
    "preprocessed_documents = df['text'].tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selection of N_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current score 0.7048616049254075, n_components = 5\n",
      "Current score 0.7546359202031336, n_components = 10\n",
      "Current score 0.7169844864297766, n_components = 15\n",
      "Current score 0.6905947006293516, n_components = 20\n",
      "Current score 0.668590728970941, n_components = 25\n",
      "Current score 0.6368263803499924, n_components = 30\n",
      "Current score 0.6401762003585644, n_components = 35\n",
      "Current score 0.6550259907441649, n_components = 40\n",
      "Current score 0.6250037375938021, n_components = 45\n",
      "Current score 0.6199270528298833, n_components = 50\n",
      "Current score 0.599805520260647, n_components = 70\n",
      "Current score 0.5922649824398365, n_components = 100\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n_components_range = [5,10,15,20,25,30,35,40,45,50,70,100]\n",
    "scores = []\n",
    "for n in n_components_range:\n",
    "    NNMFPipeline =  NNMFPipelineEnglish(preprocessed_documents,ngram_range=(1,2),n_components=n,max_iter=1000,tf_idf_max_df=0.9,tf_idf_min_df=4,random_state=0)\n",
    "    topics1 = NNMFPipeline.run_topics_detection()\n",
    "    current_score = NNMFPipeline.calculate_coherence_score()\n",
    "    print(f'Current score {current_score}, n_components = {n}')\n",
    "    scores.append(current_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_n_components = n_components_range[np.argmax(scores)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7048616049254075"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "NNMFPipeline =  NNMFPipelineEnglish(preprocessed_documents,ngram_range=(1,2),n_components=5,max_iter=1000,tf_idf_max_df=0.9,tf_idf_min_df=4,random_state=0)\n",
    "topics1 = NNMFPipeline.run_topics_detection()\n",
    "NNMFPipeline.calculate_coherence_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7562434732130454"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lsa_pipeline = LSAPipelineEnglish(preprocessed_documents,ngram_range=(1,2),lsa_components=5,svd_n_iter=200,tf_idf_max_df=0.9,tf_idf_min_df=4,random_state=0)\n",
    "topics2 = lsa_pipeline.run_topics_detection()\n",
    "lsa_pipeline.calculate_coherence_score()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling per month (not all data used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "from src.nnmf_pipeline import NNMFPipelineEnglish\n",
    "\n",
    "csv_files = [x for x in os.listdir('data/montly_data') if x.endswith('csv')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2731/3579225960.py:10: DtypeWarning: Columns (13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, usecols=usecols).drop_duplicates(subset=subset)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023_04 0.8345838584199361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2731/3579225960.py:10: DtypeWarning: Columns (13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, usecols=usecols).drop_duplicates(subset=subset)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022_08 0.7097407808017794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2731/3579225960.py:10: DtypeWarning: Columns (13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, usecols=usecols).drop_duplicates(subset=subset)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023_01 0.7420393781816056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2731/3579225960.py:10: DtypeWarning: Columns (13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, usecols=usecols).drop_duplicates(subset=subset)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023_02 0.7592571760375599\n",
      "2022_12 0.8683631982102987\n",
      "2022_11 0.8020437425581433\n",
      "2022_09 0.747827180109221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2731/3579225960.py:10: DtypeWarning: Columns (13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, usecols=usecols).drop_duplicates(subset=subset)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023_03 0.776088679856147\n",
      "2023_05 0.8019473179719212\n",
      "2022_10 0.7316282690297533\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gc  # Garbage collection\n",
    "from pathlib import Path\n",
    "\n",
    "# Assuming `NNMFPipelineEnglish` is defined elsewhere and properly imported\n",
    "\n",
    "def read_and_process_csv(file_path, usecols=['text'], subset='text'):\n",
    "    \"\"\"Read CSV file for specific columns, handle exceptions, and preprocess data.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, usecols=usecols).drop_duplicates(subset=subset)\n",
    "    except pd.errors.ParserError:\n",
    "        df = pd.read_csv(file_path, lineterminator='\\n', usecols=usecols).drop_duplicates(subset=subset)\n",
    "    df.dropna(subset=[subset], inplace=True)\n",
    "    return df['text']\n",
    "\n",
    "def extract_topics(documents):\n",
    "    \"\"\"Wrapper for NMF topic extraction and coherence score calculation.\"\"\"\n",
    "    pipeline = NNMFPipelineEnglish(documents, ngram_range=(1, 2), n_components=10, max_iter=1000,\n",
    "                                   tf_idf_max_df=0.9, tf_idf_min_df=4, random_state=0)\n",
    "    topics = pipeline.run_topics_detection()\n",
    "    score = pipeline.calculate_coherence_score()\n",
    "    return topics, score\n",
    "\n",
    "# Main processing loop\n",
    "topics_per_file = {}\n",
    "data_path = Path('data/montly_data')\n",
    "\n",
    "for csv_file in data_path.glob('*.csv'):\n",
    "    documents = read_and_process_csv(csv_file)\n",
    "    if not documents.empty:\n",
    "        topics, score = extract_topics(documents)\n",
    "        date = '_'.join(csv_file.stem.split('_')[:2])\n",
    "        print(date, score)\n",
    "        topics_per_file[date] = {'cv_score': score, 'topics': topics}\n",
    "        \n",
    "    del documents  # Explicitly delete the variable to free up memory\n",
    "    gc.collect()  # Manually trigger garbage collection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "file_path = 'topics_per_file.json'\n",
    "\n",
    "with open(file_path, 'w') as json_file:\n",
    "    json.dump(topics_per_file, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cv_score': 0.7097407808017794,\n",
       " 'topics': [['ukraine',\n",
       "   'glory',\n",
       "   'support',\n",
       "   'support ukraine',\n",
       "   'glory ukraine',\n",
       "   'day ukraine',\n",
       "   'stand',\n",
       "   'ukraine independence',\n",
       "   'russia',\n",
       "   'help'],\n",
       "  ['morning',\n",
       "   'good',\n",
       "   'good morning',\n",
       "   'twice',\n",
       "   'coming',\n",
       "   'twice coming',\n",
       "   'love',\n",
       "   'night',\n",
       "   'good night',\n",
       "   'day'],\n",
       "  ['russia',\n",
       "   'putin',\n",
       "   'state',\n",
       "   'russia defeated',\n",
       "   'possible russia',\n",
       "   'pariah state',\n",
       "   'peace possible',\n",
       "   'russia pariah',\n",
       "   'russian barbarism',\n",
       "   'barbarism limit'],\n",
       "  ['thank',\n",
       "   'support',\n",
       "   'thank support',\n",
       "   'need',\n",
       "   'support need',\n",
       "   'ukraine thank',\n",
       "   'need 20',\n",
       "   'freedom',\n",
       "   'thank fighting',\n",
       "   'independence freedom'],\n",
       "  ['day',\n",
       "   'independence',\n",
       "   'independence day',\n",
       "   'happy',\n",
       "   'happy independence',\n",
       "   'day ukraine',\n",
       "   'ukraine independence',\n",
       "   'today',\n",
       "   'ukrainian',\n",
       "   'ukrainian independence'],\n",
       "  ['analysis',\n",
       "   'im',\n",
       "   'bot',\n",
       "   'im bot',\n",
       "   'analysis article',\n",
       "   'content analysis',\n",
       "   'article score',\n",
       "   'report viewed',\n",
       "   'viewed im',\n",
       "   'complete report'],\n",
       "  ['follow',\n",
       "   'trump',\n",
       "   'trump looking',\n",
       "   'looking follow',\n",
       "   'looking',\n",
       "   'biden',\n",
       "   'link',\n",
       "   'help',\n",
       "   'follow help',\n",
       "   'help people'],\n",
       "  ['awesome',\n",
       "   'passive',\n",
       "   'long',\n",
       "   'hire',\n",
       "   'long term',\n",
       "   'term',\n",
       "   'term passive',\n",
       "   'autopilot',\n",
       "   'create',\n",
       "   'income'],\n",
       "  ['war',\n",
       "   'news',\n",
       "   'russia',\n",
       "   'today',\n",
       "   'stop',\n",
       "   'latest',\n",
       "   'russia ukraine',\n",
       "   'support',\n",
       "   'breaking news',\n",
       "   'breaking'],\n",
       "  ['russian',\n",
       "   'ukrainian',\n",
       "   'people',\n",
       "   'force',\n",
       "   'military',\n",
       "   'like',\n",
       "   'nuclear',\n",
       "   'country',\n",
       "   'region',\n",
       "   'time']]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_per_file['2022_08']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('data/processed/en_tweets_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.7048616049254076"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
