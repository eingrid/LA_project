{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "c31e1339-4e18-423c-97a4-58ccc62a351d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import json\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7c2573e-0dd2-438c-a265-be362d8dc6c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0819_UkraineCombinedTweetsDeduped.csv.gzip\tLDA.ipynb\n",
      "0819_UkraineCombinedTweetsDeduped.csv.gzip.zip\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "d2ac6ef7-a642-4109-973f-03908904eed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gzip_file_to_documents_list(file_name, languages_filter=['en']):\n",
    "    with gzip.open(file_name, 'rb') as f:\n",
    "        decompressed_data = f.read()\n",
    "    df_data = pd.read_csv(StringIO(str(decompressed_data,'utf-8')), index_col=0)\n",
    "    documents = df_data[df_data['language'].isin(languages_filter)]['text'].tolist()\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "dcb0ea35-631c-4e2a-9a5f-80a863f304ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dear vaccine advocate\n",
      "\n",
      "Do take the COVID19 mRNA shot and boosters, but do know that @OurWorldInData data shows it offers zero protection, actually accelerates death of vaccinated.\n",
      "\n",
      "Regards\n",
      "#Pfizer #AstraZeneca #Moderna #NWO #Agenda2030 #COP27 #Biden #Obama #Trudeau #Jacinda #life https://t.co/VTbfuqiDvu\n"
     ]
    }
   ],
   "source": [
    "print(gzip_file_to_documents_list(\"0819_UkraineCombinedTweetsDeduped.csv.gzip\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "e0788014-5410-48c7-999e-68d8f0a88e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uctd_file_name_by_date(month, day):\n",
    "    return '0'*int(month < 10) + str(month) + '0'*int(day < 10) + str(day) + \"_UkraineCombinedTweetsDeduped.csv.gzip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "c981afc3-ec0a-4c0d-8216-a8c524a9a285",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0821_UkraineCombinedTweetsDeduped.csv.gzip'"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uctd_file_name_by_date(8, 21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "86327671-13cc-4d35-8741-0d621754398a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uctd_documents_between_dates(start: str, end: str,\n",
    "                                     languages_filter=['en'], verbose: int = 0):\n",
    "    # Dates in the 'YYYY-DD-MM' format\n",
    "    date_start = datetime.strptime(start, '%Y-%m-%d')\n",
    "    date_end = datetime.strptime(end, '%Y-%m-%d')\n",
    "    delta_days = date_end - date_start\n",
    "    all_documents = []\n",
    "    for i_d in range(delta_days.days + 1):\n",
    "        date_current = date_start + timedelta(days=i_d)\n",
    "        uctd_file_name = uctd_file_name_by_date(date_current.month, date_current.day)\n",
    "        all_documents += gzip_file_to_documents_list(uctd_file_name, languages_filter=languages_filter)\n",
    "        if verbose == 1:\n",
    "            print(f'--Documents for the day {date_current.date()} processed')\n",
    "    return all_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "e91c6217-284a-43da-8310-089f96905ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Documents for the day 2023-08-19 processed\n",
      "--Documents for the day 2023-08-20 processed\n",
      "--Documents for the day 2023-08-21 processed\n",
      "--Documents for the day 2023-08-22 processed\n",
      "--Documents for the day 2023-08-23 processed\n"
     ]
    }
   ],
   "source": [
    "all_documents = get_uctd_documents_between_dates('2023-08-19','2023-08-23',verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "ede1c0a9-5f07-420e-9df3-0efa20c4193f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109452"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4e252c6-cbdd-42ed-a74f-24ff08a7c673",
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open(\"0819_UkraineCombinedTweetsDeduped.csv.gzip\", 'rb') as f:\n",
    "    decompressed_data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5be64cad-c6a4-419d-8977-5f51d585ab12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.read_csv(StringIO(str(decompressed_data,'utf-8')), index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "12b55ce9-93f9-4850-b22f-fd30d9a482b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['userid', 'username', 'acctdesc', 'location', 'following', 'followers',\n",
       "       'totaltweets', 'usercreatedts', 'tweetid', 'tweetcreatedts',\n",
       "       'retweetcount', 'text', 'hashtags', 'language', 'coordinates',\n",
       "       'favorite_count', 'is_retweet', 'original_tweet_id',\n",
       "       'original_tweet_userid', 'original_tweet_username',\n",
       "       'in_reply_to_status_id', 'in_reply_to_user_id',\n",
       "       'in_reply_to_screen_name', 'is_quote_status', 'quoted_status_id',\n",
       "       'quoted_status_userid', 'quoted_status_username', 'extractedts'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "546044eb-2f35-4c2f-81ea-c5f28ee2e25e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "language\n",
       "en     23179\n",
       "und     5429\n",
       "de      4226\n",
       "uk      3205\n",
       "fr      2210\n",
       "it      1898\n",
       "es      1684\n",
       "ru      1490\n",
       "ja       530\n",
       "tr       397\n",
       "pt       350\n",
       "zh       288\n",
       "in       243\n",
       "pl       218\n",
       "nl       218\n",
       "el       180\n",
       "hi       159\n",
       "ar       152\n",
       "fi       132\n",
       "ro       110\n",
       "no       102\n",
       "sv       101\n",
       "et        97\n",
       "da        96\n",
       "ca        91\n",
       "vi        87\n",
       "fa        84\n",
       "th        79\n",
       "ur        76\n",
       "ht        74\n",
       "tl        72\n",
       "cs        69\n",
       "iw        68\n",
       "bn        60\n",
       "ta        58\n",
       "lv        57\n",
       "eu        54\n",
       "ko        50\n",
       "sl        33\n",
       "bg        32\n",
       "gu        31\n",
       "ka        30\n",
       "te        29\n",
       "lt        29\n",
       "kn        27\n",
       "sr        25\n",
       "is        17\n",
       "cy        17\n",
       "ml        15\n",
       "mr        11\n",
       "am         6\n",
       "hu         4\n",
       "my         4\n",
       "or         4\n",
       "pa         3\n",
       "ne         1\n",
       "hy         1\n",
       "sd         1\n",
       "ps         1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.language.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4cbaa39a-48b4-4f28-b5e7-a1017b891976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userid</th>\n",
       "      <th>username</th>\n",
       "      <th>acctdesc</th>\n",
       "      <th>location</th>\n",
       "      <th>following</th>\n",
       "      <th>followers</th>\n",
       "      <th>totaltweets</th>\n",
       "      <th>usercreatedts</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>tweetcreatedts</th>\n",
       "      <th>...</th>\n",
       "      <th>original_tweet_userid</th>\n",
       "      <th>original_tweet_username</th>\n",
       "      <th>in_reply_to_status_id</th>\n",
       "      <th>in_reply_to_user_id</th>\n",
       "      <th>in_reply_to_screen_name</th>\n",
       "      <th>is_quote_status</th>\n",
       "      <th>quoted_status_id</th>\n",
       "      <th>quoted_status_userid</th>\n",
       "      <th>quoted_status_username</th>\n",
       "      <th>extractedts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>173212647</td>\n",
       "      <td>JoeMokolobetsi</td>\n",
       "      <td>Yeshua Hamashiach is THE answer | Romans 10:9-...</td>\n",
       "      <td>Afrika Borwa</td>\n",
       "      <td>219</td>\n",
       "      <td>197</td>\n",
       "      <td>4789</td>\n",
       "      <td>2010-07-31 19:09:22.000000</td>\n",
       "      <td>1560416252937617411</td>\n",
       "      <td>2022-08-19 00:00:00</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-08-19 08:07:26.836769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>335041409</td>\n",
       "      <td>XclusivasPuebla</td>\n",
       "      <td>Somos el periódico  #ExclusivasPuebla| Investi...</td>\n",
       "      <td>Puebla, México</td>\n",
       "      <td>1419</td>\n",
       "      <td>6402</td>\n",
       "      <td>70267</td>\n",
       "      <td>2011-07-14 02:02:24.000000</td>\n",
       "      <td>1560416256179707904</td>\n",
       "      <td>2022-08-19 00:00:01</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-08-19 07:51:50.523048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1512400441103032323</td>\n",
       "      <td>ShelterAnimalUA</td>\n",
       "      <td>Shelter for abandoned dogs and cats. 1400 dogs...</td>\n",
       "      <td>Ukraine</td>\n",
       "      <td>782</td>\n",
       "      <td>109</td>\n",
       "      <td>1198</td>\n",
       "      <td>2022-04-08 12:02:47.000000</td>\n",
       "      <td>1560416257752666113</td>\n",
       "      <td>2022-08-19 00:00:01</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-08-19 05:12:06.194216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1356632630662430722</td>\n",
       "      <td>DogandCatHelpe1</td>\n",
       "      <td>Shelter for abandoned dogs and cats. 1400 dogs...</td>\n",
       "      <td>Ukraine</td>\n",
       "      <td>5</td>\n",
       "      <td>39</td>\n",
       "      <td>690</td>\n",
       "      <td>2021-02-02 15:57:12.000000</td>\n",
       "      <td>1560416257790382081</td>\n",
       "      <td>2022-08-19 00:00:01</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-08-19 11:22:26.824532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20297125</td>\n",
       "      <td>ElMananaOnline</td>\n",
       "      <td>Las mejores noticias de los dos Laredos y el m...</td>\n",
       "      <td>Nuevo Laredo</td>\n",
       "      <td>2269</td>\n",
       "      <td>17978</td>\n",
       "      <td>56188</td>\n",
       "      <td>2009-02-07 06:32:49.000000</td>\n",
       "      <td>1560416257937051648</td>\n",
       "      <td>2022-08-19 00:00:01</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-08-19 11:52:29.448634</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                userid         username  \\\n",
       "0            173212647   JoeMokolobetsi   \n",
       "1            335041409  XclusivasPuebla   \n",
       "2  1512400441103032323  ShelterAnimalUA   \n",
       "3  1356632630662430722  DogandCatHelpe1   \n",
       "4             20297125   ElMananaOnline   \n",
       "\n",
       "                                            acctdesc        location  \\\n",
       "0  Yeshua Hamashiach is THE answer | Romans 10:9-...    Afrika Borwa   \n",
       "1  Somos el periódico  #ExclusivasPuebla| Investi...  Puebla, México   \n",
       "2  Shelter for abandoned dogs and cats. 1400 dogs...         Ukraine   \n",
       "3  Shelter for abandoned dogs and cats. 1400 dogs...         Ukraine   \n",
       "4  Las mejores noticias de los dos Laredos y el m...    Nuevo Laredo   \n",
       "\n",
       "   following  followers  totaltweets               usercreatedts  \\\n",
       "0        219        197         4789  2010-07-31 19:09:22.000000   \n",
       "1       1419       6402        70267  2011-07-14 02:02:24.000000   \n",
       "2        782        109         1198  2022-04-08 12:02:47.000000   \n",
       "3          5         39          690  2021-02-02 15:57:12.000000   \n",
       "4       2269      17978        56188  2009-02-07 06:32:49.000000   \n",
       "\n",
       "               tweetid       tweetcreatedts  ...  original_tweet_userid  \\\n",
       "0  1560416252937617411  2022-08-19 00:00:00  ...                      0   \n",
       "1  1560416256179707904  2022-08-19 00:00:01  ...                      0   \n",
       "2  1560416257752666113  2022-08-19 00:00:01  ...                      0   \n",
       "3  1560416257790382081  2022-08-19 00:00:01  ...                      0   \n",
       "4  1560416257937051648  2022-08-19 00:00:01  ...                      0   \n",
       "\n",
       "  original_tweet_username in_reply_to_status_id in_reply_to_user_id  \\\n",
       "0                     NaN                     0                   0   \n",
       "1                     NaN                     0                   0   \n",
       "2                     NaN                     0                   0   \n",
       "3                     NaN                     0                   0   \n",
       "4                     NaN                     0                   0   \n",
       "\n",
       "  in_reply_to_screen_name  is_quote_status  quoted_status_id  \\\n",
       "0                     NaN            False                 0   \n",
       "1                     NaN            False                 0   \n",
       "2                     NaN            False                 0   \n",
       "3                     NaN            False                 0   \n",
       "4                     NaN            False                 0   \n",
       "\n",
       "   quoted_status_userid  quoted_status_username                 extractedts  \n",
       "0                     0                     NaN  2022-08-19 08:07:26.836769  \n",
       "1                     0                     NaN  2022-08-19 07:51:50.523048  \n",
       "2                     0                     NaN  2022-08-19 05:12:06.194216  \n",
       "3                     0                     NaN  2022-08-19 11:22:26.824532  \n",
       "4                     0                     NaN  2022-08-19 11:52:29.448634  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0cacd7fe-d242-401d-9751-58b4f4db31ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = df_data[df_data['language']=='en']['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "f9454d12-4e7f-4d9a-af48-1516bd8866e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_uk = df_data[df_data['language']=='uk']['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "67bad359-1419-4147-a323-5dd99ed604e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dear vaccine advocate\n",
      "\n",
      "Do take the COVID19 mRNA shot and boosters, but do know that @OurWorldInData data shows it offers zero protection, actually accelerates death of vaccinated.\n",
      "\n",
      "Regards\n",
      "#Pfizer #AstraZeneca #Moderna #NWO #Agenda2030 #COP27 #Biden #Obama #Trudeau #Jacinda #life https://t.co/VTbfuqiDvu\n"
     ]
    }
   ],
   "source": [
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "04f69137-76e1-4728-9cba-75df7629c8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_links_content(text):\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    return text\n",
    "\n",
    "def remove_emails(text):\n",
    "    return re.sub('\\S+@\\S*\\s?', '', text)  # noqa\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    \"\"\"https://stackoverflow.com/a/37221663\"\"\"\n",
    "    table = str.maketrans({key: None for key in string.punctuation})\n",
    "    return text.translate(table)\n",
    "\n",
    "def remove_multiple_space(text):\n",
    "    return re.sub(\"\\s\\s+\", \" \", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "17a3c9e5-502d-4013-9b8e-03a8bc5cca40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_hashtags(text):\n",
    "    return re.sub('(?<=[\\s\\n])#\\S+\\s+', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "ac3ab58d-d504-491a-a3a4-390602d46025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dear vaccine advocate\n",
      "\n",
      "Do take the COVID19 mRNA shot and boosters, but do know that @OurWorldInData data shows it offers zero protection, actually accelerates death of vaccinated.\n",
      "\n",
      "Regards\n",
      "#Pfizer #AstraZeneca #Moderna #NWO #Agenda2030 #COP27 #Biden #Obama #Trudeau #Jacinda #life https://t.co/VTbfuqiDvu\n",
      "Dear vaccine advocate\n",
      "\n",
      "Do take the COVID19 mRNA shot and boosters, but do know that @OurWorldInData data shows it offers zero protection, actually accelerates death of vaccinated.\n",
      "\n",
      "Regards\n",
      "https://t.co/VTbfuqiDvu\n"
     ]
    }
   ],
   "source": [
    "print(documents[0])\n",
    "print(remove_hashtags(documents[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ade812df-5773-433f-b979-c3bd962f474a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "53b2d2ed-8915-4017-925a-97f16746a7cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/oleksii/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/oleksii/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/oleksii/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "1038f9f2-bcae-41be-a3af-063f1b4eb877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess documents\n",
    "def preprocess(document):\n",
    "    # Tokenize\n",
    "    document = remove_links_content(document)\n",
    "    document = remove_emails(document)\n",
    "    document = remove_hashtags(document)\n",
    "    document = remove_punctuation(document)\n",
    "    document = remove_multiple_space(document)\n",
    "    \n",
    "    words = word_tokenize(document.lower())\n",
    "    # Remove stopwords and punctuations\n",
    "    filtered_words = [word for word in words if word.isalnum() and not word in stop_words]\n",
    "    # Lemmatize\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
    "    return ' '.join(lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "0f3a0f7d-ecb8-4617-b754-bb9f1f838ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess all documents\n",
    "preprocessed_documents = [preprocess(doc) for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "3390afa7-8823-4ed8-b2c3-77d1456c0524",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=4, stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(preprocessed_documents)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "525d8a41-7c17-46e8-996b-1ff786f6df71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dear vaccine advocate take covid19 mrna shot booster know data show offer zero protection actually accelerates death vaccinated regard pfizer astrazeneca moderna nwo agenda2030 cop27 biden obama trudeau jacinda life'"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "cf92d0be-f8c3-45bf-ab19-d288a1ecb4a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4564)\t0.1395520015279304\n",
      "  (0, 7965)\t0.1864384585535873\n",
      "  (0, 5373)\t0.2023195918028436\n",
      "  (0, 1078)\t0.10189946606777062\n",
      "  (0, 1911)\t0.23615180907876085\n",
      "  (0, 5365)\t0.24162828280408494\n",
      "  (0, 5003)\t0.19323976918651925\n",
      "  (0, 5732)\t0.19173714110038736\n",
      "  (0, 6352)\t0.21983632953589854\n",
      "  (0, 8253)\t0.24162828280408494\n",
      "  (0, 2147)\t0.16033088507097837\n",
      "  (0, 346)\t0.25766948014492647\n",
      "  (0, 392)\t0.18376448998660044\n",
      "  (0, 6072)\t0.19894606194897763\n",
      "  (0, 8728)\t0.20456393935532585\n",
      "  (0, 5414)\t0.18376448998660044\n",
      "  (0, 2122)\t0.17186289387227352\n",
      "  (0, 4387)\t0.1327655200004032\n",
      "  (0, 1207)\t0.25766948014492647\n",
      "  (0, 7030)\t0.1711660683730146\n",
      "  (0, 5075)\t0.2526937699992935\n",
      "  (0, 1970)\t0.1770274687812375\n",
      "  (0, 438)\t0.23615180907876085\n",
      "  (0, 8255)\t0.17186289387227352\n",
      "  (0, 2146)\t0.1872531712745243\n"
     ]
    }
   ],
   "source": [
    "print(tfidf[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "35eecb14-a15d-4169-9a4a-86bc3d838575",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "188a525e-bdfd-4de9-9609-d3302f0fe9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TruncatedSVD(n_components=100, n_iter=5, random_state=47)\n",
    "model.fit_transform(tfidf)\n",
    "model_components = model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "17c76c43-72e4-44bb-a7b3-d9add3b9c4e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01098185, 0.00711174, 0.00545351, 0.0048956 , 0.0043829 ])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.explained_variance_ratio_[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "2441a42a-0a39-4e77-87c8-2283ceec88e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.39232363e-04, 2.33241475e-04, 2.25998857e-04, ...,\n",
       "       1.59070021e-04, 2.07269724e-05, 2.33960833e-04])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_components[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "0939a93f-054a-4628-8c24-a5a9b8c017a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the top words for each topic\n",
    "n_top_words = 10\n",
    "topics = []\n",
    "for topic_idx, topic in enumerate(model_components):\n",
    "    top_features_ind = topic.argsort()[:-n_top_words - 1:-1]\n",
    "    top_features = [tfidf_feature_names[i] for i in top_features_ind]\n",
    "    topics.append(top_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "6fa0d864-d25c-42a0-b56d-0dc49df91f43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ukraine',\n",
       "  'russia',\n",
       "  'russian',\n",
       "  'war',\n",
       "  'ukrainian',\n",
       "  'putin',\n",
       "  'day',\n",
       "  'state',\n",
       "  'amp',\n",
       "  'people'],\n",
       " ['russia',\n",
       "  'russian',\n",
       "  'putin',\n",
       "  'ukrainian',\n",
       "  'state',\n",
       "  'pariah',\n",
       "  'barbarism',\n",
       "  'limit',\n",
       "  'defeated',\n",
       "  'crimeabelongstoukraine'],\n",
       " ['awesome',\n",
       "  'passive',\n",
       "  'hire',\n",
       "  'website',\n",
       "  'income',\n",
       "  'create',\n",
       "  'term',\n",
       "  'autopilot',\n",
       "  'long',\n",
       "  'developer'],\n",
       " ['russia',\n",
       "  'war',\n",
       "  'putin',\n",
       "  'news',\n",
       "  'stop',\n",
       "  'ukraine',\n",
       "  'latest',\n",
       "  'state',\n",
       "  'breaking',\n",
       "  'peace'],\n",
       " ['putin',\n",
       "  'daughter',\n",
       "  'war',\n",
       "  'car',\n",
       "  'killed',\n",
       "  'amp',\n",
       "  'dugin',\n",
       "  'bomb',\n",
       "  'alexander',\n",
       "  'moscow'],\n",
       " ['day',\n",
       "  'flag',\n",
       "  'putin',\n",
       "  'happy',\n",
       "  'amp',\n",
       "  'independence',\n",
       "  'national',\n",
       "  'daughter',\n",
       "  'russia',\n",
       "  'car'],\n",
       " ['ukraine',\n",
       "  'putin',\n",
       "  'daughter',\n",
       "  'car',\n",
       "  'killed',\n",
       "  'fuck',\n",
       "  'dugin',\n",
       "  'bomb',\n",
       "  'moscow',\n",
       "  'alexander'],\n",
       " ['war',\n",
       "  'russian',\n",
       "  'day',\n",
       "  'putin',\n",
       "  'stop',\n",
       "  'flag',\n",
       "  'ukrainian',\n",
       "  'happy',\n",
       "  'support',\n",
       "  'independence'],\n",
       " ['standwithukraine',\n",
       "  'playing',\n",
       "  'video',\n",
       "  'putin',\n",
       "  'war',\n",
       "  'stop',\n",
       "  'peace',\n",
       "  'ukrainian',\n",
       "  'fuck',\n",
       "  'rock'],\n",
       " ['war',\n",
       "  'standwithukraine',\n",
       "  'video',\n",
       "  'news',\n",
       "  'russia',\n",
       "  'daughter',\n",
       "  'playing',\n",
       "  'breaking',\n",
       "  'today',\n",
       "  'car'],\n",
       " ['follow',\n",
       "  'trump',\n",
       "  'looking',\n",
       "  'news',\n",
       "  'war',\n",
       "  'people',\n",
       "  'article',\n",
       "  'help',\n",
       "  'link',\n",
       "  'today'],\n",
       " ['putin',\n",
       "  'nuclear',\n",
       "  'plant',\n",
       "  'power',\n",
       "  'fuck',\n",
       "  'russian',\n",
       "  'zaporizhzhia',\n",
       "  'military',\n",
       "  'morning',\n",
       "  'good'],\n",
       " ['amp',\n",
       "  'russian',\n",
       "  'news',\n",
       "  'breaking',\n",
       "  'trump',\n",
       "  'latest',\n",
       "  'today',\n",
       "  'ukraine',\n",
       "  'looking',\n",
       "  'follow'],\n",
       " ['ukrainian',\n",
       "  'news',\n",
       "  'amp',\n",
       "  'today',\n",
       "  'breaking',\n",
       "  'latest',\n",
       "  'force',\n",
       "  'new',\n",
       "  'putin',\n",
       "  'peace'],\n",
       " ['good',\n",
       "  'morning',\n",
       "  'news',\n",
       "  'today',\n",
       "  'like',\n",
       "  'latest',\n",
       "  'breaking',\n",
       "  'peace',\n",
       "  'new',\n",
       "  'thank'],\n",
       " ['analysis',\n",
       "  'gosloto',\n",
       "  'stats',\n",
       "  'im',\n",
       "  'result',\n",
       "  'report',\n",
       "  'article',\n",
       "  'bot',\n",
       "  'content',\n",
       "  'viewed'],\n",
       " ['new',\n",
       "  'peace',\n",
       "  'check',\n",
       "  'nuclear',\n",
       "  'plant',\n",
       "  'uploaded',\n",
       "  'power',\n",
       "  'good',\n",
       "  'daughter',\n",
       "  'car'],\n",
       " ['morning',\n",
       "  'good',\n",
       "  'nuclear',\n",
       "  'amp',\n",
       "  'plant',\n",
       "  'ukrainian',\n",
       "  'stop',\n",
       "  'power',\n",
       "  'analysis',\n",
       "  'support'],\n",
       " ['news',\n",
       "  'today',\n",
       "  'nuclear',\n",
       "  'world',\n",
       "  'latest',\n",
       "  'breaking',\n",
       "  'plant',\n",
       "  'russian',\n",
       "  'power',\n",
       "  'like'],\n",
       " ['support',\n",
       "  'stop',\n",
       "  'today',\n",
       "  'news',\n",
       "  'thank',\n",
       "  'breaking',\n",
       "  'people',\n",
       "  'latest',\n",
       "  'plant',\n",
       "  'attack'],\n",
       " ['russiaisaterroriststate',\n",
       "  'terrorist',\n",
       "  'state',\n",
       "  'news',\n",
       "  'new',\n",
       "  'ukrainian',\n",
       "  'breaking',\n",
       "  'today',\n",
       "  'check',\n",
       "  'latest'],\n",
       " ['like',\n",
       "  'look',\n",
       "  'crimea',\n",
       "  'stop',\n",
       "  'support',\n",
       "  'drone',\n",
       "  'air',\n",
       "  'nuclear',\n",
       "  'plant',\n",
       "  'flag'],\n",
       " ['people',\n",
       "  'attack',\n",
       "  'state',\n",
       "  'missile',\n",
       "  'war',\n",
       "  'say',\n",
       "  'terrorist',\n",
       "  'nuclear',\n",
       "  'civilian',\n",
       "  'plant'],\n",
       " ['thank',\n",
       "  'state',\n",
       "  'war',\n",
       "  'attack',\n",
       "  'crimea',\n",
       "  'terrorist',\n",
       "  'say',\n",
       "  'possible',\n",
       "  'nuclear',\n",
       "  'president'],\n",
       " ['new',\n",
       "  'russia',\n",
       "  'result',\n",
       "  'thank',\n",
       "  'people',\n",
       "  'gosloto',\n",
       "  'stats',\n",
       "  'russiaisaterroriststate',\n",
       "  '536',\n",
       "  'russian'],\n",
       " ['result',\n",
       "  'gosloto',\n",
       "  'stats',\n",
       "  '536',\n",
       "  'world',\n",
       "  'state',\n",
       "  'analysis',\n",
       "  'country',\n",
       "  'attack',\n",
       "  'terrorist'],\n",
       " ['crimea',\n",
       "  'flag',\n",
       "  'black',\n",
       "  'sea',\n",
       "  'russiaisaterroriststate',\n",
       "  'world',\n",
       "  'fleet',\n",
       "  'drone',\n",
       "  'explosion',\n",
       "  'people'],\n",
       " ['force',\n",
       "  'world',\n",
       "  'people',\n",
       "  'like',\n",
       "  'armed',\n",
       "  'region',\n",
       "  'russiaisaterroriststate',\n",
       "  'thank',\n",
       "  'update',\n",
       "  'largest'],\n",
       " ['help',\n",
       "  'need',\n",
       "  'save',\n",
       "  'weapon',\n",
       "  'defender',\n",
       "  'day',\n",
       "  'stop',\n",
       "  'like',\n",
       "  'send',\n",
       "  'region'],\n",
       " ['russiaisaterroriststate',\n",
       "  'support',\n",
       "  'say',\n",
       "  'know',\n",
       "  'day',\n",
       "  'time',\n",
       "  'military',\n",
       "  'independence',\n",
       "  'possible',\n",
       "  'good'],\n",
       " ['flag',\n",
       "  'need',\n",
       "  'support',\n",
       "  'morning',\n",
       "  'happy',\n",
       "  'national',\n",
       "  'weapon',\n",
       "  'war',\n",
       "  'like',\n",
       "  'fuck'],\n",
       " ['force',\n",
       "  'flag',\n",
       "  'time',\n",
       "  'love',\n",
       "  'know',\n",
       "  'good',\n",
       "  'armed',\n",
       "  'happy',\n",
       "  'people',\n",
       "  'national'],\n",
       " ['country',\n",
       "  'defense',\n",
       "  'air',\n",
       "  'save',\n",
       "  'russiaisaterroriststate',\n",
       "  'germany',\n",
       "  'missile',\n",
       "  'life',\n",
       "  'good',\n",
       "  'defender'],\n",
       " ['morning',\n",
       "  'say',\n",
       "  'know',\n",
       "  'time',\n",
       "  'biden',\n",
       "  'stop',\n",
       "  'president',\n",
       "  'russiaisaterroriststate',\n",
       "  'love',\n",
       "  'want'],\n",
       " ['military',\n",
       "  'biden',\n",
       "  'president',\n",
       "  'people',\n",
       "  'aid',\n",
       "  'flag',\n",
       "  'state',\n",
       "  'million',\n",
       "  'said',\n",
       "  'crimea'],\n",
       " ['time',\n",
       "  'support',\n",
       "  'update',\n",
       "  'region',\n",
       "  'fuck',\n",
       "  'august',\n",
       "  'terrorist',\n",
       "  'tension',\n",
       "  'help',\n",
       "  'state'],\n",
       " ['time',\n",
       "  'say',\n",
       "  'today',\n",
       "  'attack',\n",
       "  'missile',\n",
       "  'world',\n",
       "  'need',\n",
       "  'thank',\n",
       "  'civilian',\n",
       "  'flag'],\n",
       " ['love',\n",
       "  'military',\n",
       "  'need',\n",
       "  'missile',\n",
       "  'defense',\n",
       "  'air',\n",
       "  'aid',\n",
       "  'today',\n",
       "  'time',\n",
       "  'slavaukraini'],\n",
       " ['know',\n",
       "  'country',\n",
       "  'region',\n",
       "  'update',\n",
       "  'military',\n",
       "  'tension',\n",
       "  'thank',\n",
       "  'child',\n",
       "  'missile',\n",
       "  'need'],\n",
       " ['update',\n",
       "  'say',\n",
       "  'region',\n",
       "  'love',\n",
       "  'tension',\n",
       "  'world',\n",
       "  'good',\n",
       "  'save',\n",
       "  'flag',\n",
       "  'rising'],\n",
       " ['know',\n",
       "  'air',\n",
       "  'defense',\n",
       "  'military',\n",
       "  'save',\n",
       "  'defender',\n",
       "  'fuck',\n",
       "  'state',\n",
       "  'want',\n",
       "  'help'],\n",
       " ['need',\n",
       "  'air',\n",
       "  'defense',\n",
       "  'president',\n",
       "  'world',\n",
       "  'news',\n",
       "  'people',\n",
       "  'fuck',\n",
       "  'biden',\n",
       "  'morning'],\n",
       " ['today',\n",
       "  'month',\n",
       "  'president',\n",
       "  'biden',\n",
       "  'max',\n",
       "  'min',\n",
       "  'air',\n",
       "  'good',\n",
       "  'year',\n",
       "  'world'],\n",
       " ['attack',\n",
       "  'biden',\n",
       "  'president',\n",
       "  'news',\n",
       "  'missile',\n",
       "  'save',\n",
       "  'flag',\n",
       "  'know',\n",
       "  'russiaisaterroriststate',\n",
       "  'happy'],\n",
       " ['fuck',\n",
       "  'biden',\n",
       "  'want',\n",
       "  'stop',\n",
       "  'innifsek',\n",
       "  'mur',\n",
       "  'lilek',\n",
       "  'maltese',\n",
       "  'english',\n",
       "  'peace'],\n",
       " ['want',\n",
       "  'dont',\n",
       "  'attack',\n",
       "  'video',\n",
       "  'air',\n",
       "  'putin',\n",
       "  'make',\n",
       "  'update',\n",
       "  'defense',\n",
       "  'region'],\n",
       " ['biden',\n",
       "  'hit',\n",
       "  'black',\n",
       "  'video',\n",
       "  'new',\n",
       "  'sea',\n",
       "  'drone',\n",
       "  'missile',\n",
       "  'fleet',\n",
       "  'child'],\n",
       " ['update',\n",
       "  'new',\n",
       "  'drone',\n",
       "  'black',\n",
       "  'sea',\n",
       "  'air',\n",
       "  'help',\n",
       "  'fleet',\n",
       "  'dugin',\n",
       "  'biden'],\n",
       " ['help',\n",
       "  'video',\n",
       "  'country',\n",
       "  'link',\n",
       "  'attack',\n",
       "  'soldier',\n",
       "  'check',\n",
       "  'peace',\n",
       "  'explosion',\n",
       "  'china'],\n",
       " ['august',\n",
       "  '2022',\n",
       "  'year',\n",
       "  'flag',\n",
       "  '22',\n",
       "  'child',\n",
       "  '23',\n",
       "  'china',\n",
       "  'black',\n",
       "  'sea'],\n",
       " ['president',\n",
       "  'want',\n",
       "  'time',\n",
       "  'know',\n",
       "  'fuck',\n",
       "  'country',\n",
       "  'love',\n",
       "  'good',\n",
       "  'like',\n",
       "  'missile'],\n",
       " ['biden',\n",
       "  'update',\n",
       "  'check',\n",
       "  'video',\n",
       "  'military',\n",
       "  'save',\n",
       "  'peace',\n",
       "  'terrorist',\n",
       "  'great',\n",
       "  'country'],\n",
       " ['slavaukraini',\n",
       "  'president',\n",
       "  'video',\n",
       "  'dugin',\n",
       "  'explosion',\n",
       "  'said',\n",
       "  'fuck',\n",
       "  'defender',\n",
       "  'right',\n",
       "  'new'],\n",
       " ['slavaukraini',\n",
       "  'president',\n",
       "  'said',\n",
       "  'black',\n",
       "  'sea',\n",
       "  'drone',\n",
       "  'check',\n",
       "  'fleet',\n",
       "  'peace',\n",
       "  'standwithukraine'],\n",
       " ['nato',\n",
       "  'right',\n",
       "  'weapon',\n",
       "  'president',\n",
       "  'think',\n",
       "  'soldier',\n",
       "  'drone',\n",
       "  'said',\n",
       "  'video',\n",
       "  'army'],\n",
       " ['video',\n",
       "  'stop',\n",
       "  'crimea',\n",
       "  'update',\n",
       "  'soldier',\n",
       "  'glory',\n",
       "  'weapon',\n",
       "  'slavaukraini',\n",
       "  'new',\n",
       "  'money'],\n",
       " ['nato',\n",
       "  'slavaukraini',\n",
       "  'new',\n",
       "  'august',\n",
       "  'missile',\n",
       "  'biden',\n",
       "  'killed',\n",
       "  'crimea',\n",
       "  'help',\n",
       "  '2022'],\n",
       " ['explosion',\n",
       "  'slavaukraini',\n",
       "  'glory',\n",
       "  'soldier',\n",
       "  'attack',\n",
       "  'city',\n",
       "  'right',\n",
       "  'nato',\n",
       "  'near',\n",
       "  'air'],\n",
       " ['right',\n",
       "  'stop',\n",
       "  'missile',\n",
       "  'august',\n",
       "  'strike',\n",
       "  'help',\n",
       "  'country',\n",
       "  'president',\n",
       "  '2022',\n",
       "  'think'],\n",
       " ['soldier',\n",
       "  'right',\n",
       "  'glory',\n",
       "  'tell',\n",
       "  'child',\n",
       "  'sign',\n",
       "  'fuck',\n",
       "  'update',\n",
       "  'petition',\n",
       "  'state'],\n",
       " ['nazi',\n",
       "  'child',\n",
       "  'right',\n",
       "  'slavaukraini',\n",
       "  'video',\n",
       "  'happy',\n",
       "  'year',\n",
       "  'kill',\n",
       "  'nato',\n",
       "  'biden'],\n",
       " ['nazi',\n",
       "  'think',\n",
       "  'explosion',\n",
       "  'glory',\n",
       "  'let',\n",
       "  'child',\n",
       "  'attack',\n",
       "  'dugin',\n",
       "  'soldier',\n",
       "  'great'],\n",
       " ['great',\n",
       "  'make',\n",
       "  'thanks',\n",
       "  'work',\n",
       "  'child',\n",
       "  'want',\n",
       "  'stop',\n",
       "  'know',\n",
       "  'nato',\n",
       "  'video'],\n",
       " ['glory',\n",
       "  'nazi',\n",
       "  'dugin',\n",
       "  'soldier',\n",
       "  'china',\n",
       "  'missile',\n",
       "  'region',\n",
       "  'alexander',\n",
       "  'news',\n",
       "  'ukrainerussiawar'],\n",
       " ['army',\n",
       "  'nazi',\n",
       "  'weapon',\n",
       "  'slavaukraini',\n",
       "  'footage',\n",
       "  'said',\n",
       "  'money',\n",
       "  'know',\n",
       "  'right',\n",
       "  'national'],\n",
       " ['happy',\n",
       "  'make',\n",
       "  'weapon',\n",
       "  'money',\n",
       "  'august',\n",
       "  'send',\n",
       "  'world',\n",
       "  'let',\n",
       "  'independence',\n",
       "  'soldier'],\n",
       " ['happy',\n",
       "  'army',\n",
       "  'dugin',\n",
       "  'footage',\n",
       "  'independence',\n",
       "  'month',\n",
       "  'need',\n",
       "  'right',\n",
       "  'line',\n",
       "  'china'],\n",
       " ['nazi',\n",
       "  'blame',\n",
       "  'problem',\n",
       "  'soldier',\n",
       "  'happening',\n",
       "  'border',\n",
       "  'whats',\n",
       "  'play',\n",
       "  'issue',\n",
       "  'maga'],\n",
       " ['make',\n",
       "  'glory',\n",
       "  'money',\n",
       "  'region',\n",
       "  'army',\n",
       "  'need',\n",
       "  'month',\n",
       "  'let',\n",
       "  'blame',\n",
       "  'border'],\n",
       " ['region',\n",
       "  'month',\n",
       "  'news',\n",
       "  'think',\n",
       "  'great',\n",
       "  'right',\n",
       "  'end',\n",
       "  'max',\n",
       "  'min',\n",
       "  'invasion'],\n",
       " ['nazi',\n",
       "  'think',\n",
       "  'killed',\n",
       "  'glory',\n",
       "  'china',\n",
       "  'president',\n",
       "  'car',\n",
       "  'missile',\n",
       "  'money',\n",
       "  'bomb'],\n",
       " ['let',\n",
       "  'dont',\n",
       "  'nazi',\n",
       "  'end',\n",
       "  'month',\n",
       "  'killed',\n",
       "  'great',\n",
       "  'news',\n",
       "  'europe',\n",
       "  'fight'],\n",
       " ['make',\n",
       "  'said',\n",
       "  'think',\n",
       "  'soldier',\n",
       "  'europe',\n",
       "  'leave',\n",
       "  'missile',\n",
       "  'largest',\n",
       "  'month',\n",
       "  'civilian'],\n",
       " ['glory',\n",
       "  'explosion',\n",
       "  'month',\n",
       "  'update',\n",
       "  'news',\n",
       "  'army',\n",
       "  'kill',\n",
       "  'civilian',\n",
       "  'child',\n",
       "  'missile'],\n",
       " ['said',\n",
       "  'glory',\n",
       "  'happy',\n",
       "  'thanks',\n",
       "  'dont',\n",
       "  'today',\n",
       "  'independence',\n",
       "  'region',\n",
       "  'breaking',\n",
       "  'work'],\n",
       " ['dont',\n",
       "  'work',\n",
       "  'year',\n",
       "  'great',\n",
       "  'money',\n",
       "  'nato',\n",
       "  'forget',\n",
       "  'president',\n",
       "  'family',\n",
       "  'crime'],\n",
       " ['year',\n",
       "  'invasion',\n",
       "  'kyiv',\n",
       "  'end',\n",
       "  'stand',\n",
       "  'destroyed',\n",
       "  'explosion',\n",
       "  'tank',\n",
       "  'said',\n",
       "  'kill'],\n",
       " ['end',\n",
       "  'europe',\n",
       "  'gas',\n",
       "  'air',\n",
       "  'civilian',\n",
       "  'strike',\n",
       "  'weapon',\n",
       "  'largest',\n",
       "  'win',\n",
       "  'kyiv'],\n",
       " ['year',\n",
       "  'let',\n",
       "  'thanks',\n",
       "  'breaking',\n",
       "  'europe',\n",
       "  'latest',\n",
       "  'leave',\n",
       "  'kill',\n",
       "  'fab',\n",
       "  'largest'],\n",
       " ['end',\n",
       "  'said',\n",
       "  'dont',\n",
       "  'dugin',\n",
       "  'life',\n",
       "  'fab',\n",
       "  'killed',\n",
       "  'city',\n",
       "  'latest',\n",
       "  'celebritymasterchef'],\n",
       " ['city',\n",
       "  'year',\n",
       "  'win',\n",
       "  'strike',\n",
       "  'thanks',\n",
       "  'kharkiv',\n",
       "  'leave',\n",
       "  'hit',\n",
       "  'going',\n",
       "  'fight'],\n",
       " ['work',\n",
       "  'way',\n",
       "  'end',\n",
       "  'weapon',\n",
       "  'life',\n",
       "  'home',\n",
       "  'service',\n",
       "  'tank',\n",
       "  'family',\n",
       "  'best'],\n",
       " ['kill',\n",
       "  'end',\n",
       "  'fab',\n",
       "  'way',\n",
       "  'celebritymasterchef',\n",
       "  'car',\n",
       "  'look',\n",
       "  'make',\n",
       "  'china',\n",
       "  'work'],\n",
       " ['way',\n",
       "  'fab',\n",
       "  'celebritymasterchef',\n",
       "  'ukrainerussiawar',\n",
       "  'money',\n",
       "  'life',\n",
       "  'going',\n",
       "  'kill',\n",
       "  'work',\n",
       "  'thread'],\n",
       " ['work',\n",
       "  'end',\n",
       "  'money',\n",
       "  'invasion',\n",
       "  'kill',\n",
       "  'region',\n",
       "  'going',\n",
       "  'civilian',\n",
       "  'china',\n",
       "  'thanks'],\n",
       " ['thread',\n",
       "  'thanks',\n",
       "  'read',\n",
       "  'going',\n",
       "  'fab',\n",
       "  'celebritymasterchef',\n",
       "  'hit',\n",
       "  'dugina',\n",
       "  'crime',\n",
       "  'stand'],\n",
       " ['win',\n",
       "  'work',\n",
       "  'said',\n",
       "  'make',\n",
       "  'read',\n",
       "  'thread',\n",
       "  'dugin',\n",
       "  'dont',\n",
       "  'kill',\n",
       "  'link'],\n",
       " ['way',\n",
       "  'money',\n",
       "  'invasion',\n",
       "  'thanks',\n",
       "  'said',\n",
       "  'missile',\n",
       "  'killed',\n",
       "  'fab',\n",
       "  'stand',\n",
       "  'celebritymasterchef'],\n",
       " ['money',\n",
       "  'tank',\n",
       "  'destroyed',\n",
       "  'thanks',\n",
       "  'end',\n",
       "  'work',\n",
       "  'said',\n",
       "  'kill',\n",
       "  'going',\n",
       "  'month'],\n",
       " ['win',\n",
       "  'life',\n",
       "  'fab',\n",
       "  'link',\n",
       "  'end',\n",
       "  'celebritymasterchef',\n",
       "  'crime',\n",
       "  'money',\n",
       "  'stand',\n",
       "  'service'],\n",
       " ['going',\n",
       "  'work',\n",
       "  'killed',\n",
       "  'breaking',\n",
       "  'explosion',\n",
       "  'thing',\n",
       "  'missile',\n",
       "  'civilian',\n",
       "  'leave',\n",
       "  'strike'],\n",
       " ['fight',\n",
       "  'ukrainerussiawar',\n",
       "  'killed',\n",
       "  'fab',\n",
       "  'invasion',\n",
       "  'celebritymasterchef',\n",
       "  'free',\n",
       "  'look',\n",
       "  'state',\n",
       "  'dugin'],\n",
       " ['thanks',\n",
       "  'ukrainerussiawar',\n",
       "  'best',\n",
       "  'friend',\n",
       "  'news',\n",
       "  'moscow',\n",
       "  'guy',\n",
       "  'state',\n",
       "  'dugin',\n",
       "  'germany'],\n",
       " ['aid',\n",
       "  'security',\n",
       "  'million',\n",
       "  'billion',\n",
       "  'city',\n",
       "  'ukrainerussiawar',\n",
       "  'drone',\n",
       "  'package',\n",
       "  'near',\n",
       "  'kill'],\n",
       " ['kill',\n",
       "  'city',\n",
       "  'killed',\n",
       "  'gas',\n",
       "  'best',\n",
       "  'thing',\n",
       "  'friend',\n",
       "  'free',\n",
       "  'daughter',\n",
       "  'medium'],\n",
       " ['month',\n",
       "  'breaking',\n",
       "  'state',\n",
       "  'thanks',\n",
       "  'killed',\n",
       "  'stand',\n",
       "  'life',\n",
       "  'look',\n",
       "  'special',\n",
       "  'kyiv'],\n",
       " ['ukrainerussiawar',\n",
       "  'invasion',\n",
       "  'god',\n",
       "  'dugina',\n",
       "  'visit',\n",
       "  'going',\n",
       "  'home',\n",
       "  'german',\n",
       "  'send',\n",
       "  'drone'],\n",
       " ['state',\n",
       "  'read',\n",
       "  'independence',\n",
       "  'missile',\n",
       "  'link',\n",
       "  'best',\n",
       "  'check',\n",
       "  'child',\n",
       "  'free',\n",
       "  'security'],\n",
       " ['friend',\n",
       "  'home',\n",
       "  'best',\n",
       "  'family',\n",
       "  'near',\n",
       "  'hit',\n",
       "  'come',\n",
       "  'thing',\n",
       "  'month',\n",
       "  'latest'],\n",
       " ['really',\n",
       "  'read',\n",
       "  'ukrainerussiawar',\n",
       "  'free',\n",
       "  'yes',\n",
       "  'breaking',\n",
       "  'come',\n",
       "  'game',\n",
       "  'strike',\n",
       "  'terrorist']]"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "1d3e0291-6c3b-4193-b298-0b971352606a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Score: 0.6986271061959081\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim import matutils\n",
    "\n",
    "# Convert the list of top words into a list of lists of words\n",
    "texts = [[word for word in doc.lower().split() if word in tfidf_feature_names] for doc in preprocessed_documents]\n",
    "\n",
    "# Create a Gensim dictionary\n",
    "dictionary = Dictionary(texts)\n",
    "\n",
    "# Convert the dictionary and the corpus\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# Calculate the coherence score using Gensim\n",
    "coherence_model = CoherenceModel(topics=topics, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "coherence_score = coherence_model.get_coherence()\n",
    "\n",
    "print('Coherence Score:', coherence_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "0b6cf1ab-474e-4c56-b5f7-17ad97d115f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Score: 0.7877023306141202\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim import matutils\n",
    "\n",
    "# Convert the list of top words into a list of lists of words\n",
    "texts = [[word for word in doc.lower().split() if word in tfidf_feature_names] for doc in preprocessed_documents]\n",
    "\n",
    "# Create a Gensim dictionary\n",
    "dictionary = Dictionary(texts)\n",
    "\n",
    "# Convert the dictionary and the corpus\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# Calculate the coherence score using Gensim\n",
    "coherence_model = CoherenceModel(topics=topics, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "coherence_score = coherence_model.get_coherence()\n",
    "\n",
    "print('Coherence Score:', coherence_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8c5b6e-106f-4caf-9ac6-0871db701d56",
   "metadata": {},
   "source": [
    "## All documents between 19.08 and 23.08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "cfef8a0b-2d14-4d9c-91cf-aab3cbd77b0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ukraine',\n",
       "  'russia',\n",
       "  'russian',\n",
       "  'war',\n",
       "  'ukrainian',\n",
       "  'putin',\n",
       "  'day',\n",
       "  'state',\n",
       "  'amp',\n",
       "  'people'],\n",
       " ['russia',\n",
       "  'russian',\n",
       "  'putin',\n",
       "  'ukrainian',\n",
       "  'state',\n",
       "  'pariah',\n",
       "  'barbarism',\n",
       "  'limit',\n",
       "  'defeated',\n",
       "  'crimeabelongstoukraine'],\n",
       " ['awesome',\n",
       "  'passive',\n",
       "  'hire',\n",
       "  'website',\n",
       "  'income',\n",
       "  'create',\n",
       "  'term',\n",
       "  'autopilot',\n",
       "  'long',\n",
       "  'developer'],\n",
       " ['russia',\n",
       "  'war',\n",
       "  'putin',\n",
       "  'news',\n",
       "  'stop',\n",
       "  'ukraine',\n",
       "  'latest',\n",
       "  'state',\n",
       "  'breaking',\n",
       "  'support'],\n",
       " ['putin',\n",
       "  'daughter',\n",
       "  'war',\n",
       "  'car',\n",
       "  'amp',\n",
       "  'killed',\n",
       "  'dugin',\n",
       "  'bomb',\n",
       "  'alexander',\n",
       "  'moscow'],\n",
       " ['day',\n",
       "  'putin',\n",
       "  'flag',\n",
       "  'happy',\n",
       "  'daughter',\n",
       "  'independence',\n",
       "  'car',\n",
       "  'national',\n",
       "  'killed',\n",
       "  'fuck'],\n",
       " ['ukraine',\n",
       "  'russian',\n",
       "  'putin',\n",
       "  'daughter',\n",
       "  'car',\n",
       "  'killed',\n",
       "  'dugin',\n",
       "  'bomb',\n",
       "  'alexander',\n",
       "  'moscow'],\n",
       " ['war',\n",
       "  'russian',\n",
       "  'day',\n",
       "  'putin',\n",
       "  'stop',\n",
       "  'flag',\n",
       "  'happy',\n",
       "  'ukrainian',\n",
       "  'support',\n",
       "  'today'],\n",
       " ['standwithukraine',\n",
       "  'playing',\n",
       "  'video',\n",
       "  'putin',\n",
       "  'war',\n",
       "  'stop',\n",
       "  'russian',\n",
       "  'peace',\n",
       "  'barbarism',\n",
       "  'pariah'],\n",
       " ['war',\n",
       "  'standwithukraine',\n",
       "  'video',\n",
       "  'news',\n",
       "  'russia',\n",
       "  'playing',\n",
       "  'daughter',\n",
       "  'car',\n",
       "  'breaking',\n",
       "  'today']]"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocess all documents\n",
    "preprocessed_all_documents = [preprocess(doc) for doc in all_documents]\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.9, min_df=4, stop_words='english') # 0.95\n",
    "tfidf = tfidf_vectorizer.fit_transform(preprocessed_all_documents)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "model = TruncatedSVD(n_components=10, n_iter=5, random_state=47)\n",
    "model.fit_transform(tfidf)\n",
    "model_components = model.components_\n",
    "\n",
    "# Extract the top words for each topic\n",
    "n_top_words = 10\n",
    "topics = []\n",
    "for topic_idx, topic in enumerate(model_components):\n",
    "    top_features_ind = topic.argsort()[:-n_top_words - 1:-1]\n",
    "    top_features = [tfidf_feature_names[i] for i in top_features_ind]\n",
    "    topics.append(top_features)\n",
    "\n",
    "topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "a5df7a24-3787-41b0-a7ed-b3b6332a2770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Score: 0.4904281454464573\n"
     ]
    }
   ],
   "source": [
    "# Convert the list of top words into a list of lists of words\n",
    "texts = [[word for word in doc.lower().split() if word in tfidf_feature_names] for doc in preprocessed_documents]\n",
    "\n",
    "# Create a Gensim dictionary\n",
    "dictionary = Dictionary(texts)\n",
    "\n",
    "# Convert the dictionary and the corpus\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# Calculate the coherence score using Gensim\n",
    "coherence_model = CoherenceModel(topics=topics, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "coherence_score = coherence_model.get_coherence()\n",
    "\n",
    "print('Coherence Score:', coherence_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b703d9-46b0-4f7f-9d27-59cae2102621",
   "metadata": {},
   "source": [
    "# General processing pipeline\n",
    "\n",
    "## English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "29cd50fb-e822-4db9-9320-cc692e684553",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnglishPreprocessor:\n",
    "    def preprocess_documents(self, documents_list: list, return_strings: bool = True):\n",
    "        return [self.preprocess(doc, return_string=return_strings) for doc in documents_list]\n",
    "    \n",
    "    # Function to preprocess documents\n",
    "    def preprocess(self, document, return_string: bool = True):\n",
    "        # Tokenize\n",
    "        document = self.remove_links_content(document)\n",
    "        document = self.remove_emails(document)\n",
    "        document = self.remove_multiple_space(document)\n",
    "        document = self.remove_hashtags(document)\n",
    "        document = self.remove_punctuation(document)\n",
    "        document = self.remove_multiple_space(document)\n",
    "        \n",
    "        words = word_tokenize(document.lower())\n",
    "        # Remove stopwords and punctuations\n",
    "        filtered_words = [word for word in words if word.isalnum() and not word in stop_words]\n",
    "        # Lemmatize\n",
    "        lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
    "        if return_string:\n",
    "            return ' '.join(lemmatized_words)\n",
    "        else:\n",
    "            return lemmatized_words\n",
    "    \n",
    "    def remove_links_content(self, text):\n",
    "        text = re.sub(r\"http\\S+\", \"\", text)\n",
    "        return text\n",
    "    \n",
    "    def remove_emails(self, text):\n",
    "        return re.sub('\\S+@\\S*\\s?', '', text)\n",
    "    \n",
    "    def remove_punctuation(self, text):\n",
    "        \"\"\"https://stackoverflow.com/a/37221663\"\"\"\n",
    "        table = str.maketrans({key: None for key in string.punctuation})\n",
    "        return text.translate(table)\n",
    "    \n",
    "    def remove_multiple_space(self, text):\n",
    "        return re.sub(\"\\s\\s+\", \" \", text)\n",
    "\n",
    "    def remove_hashtags(self, text):\n",
    "        return re.sub('(?<=[\\s\\n])#\\S+\\s+', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "dca2d7f3-c3fe-43f1-a70d-1763753f0dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSAPipelineEnglish:\n",
    "    def __init__(self, documents_list, tf_idf_max_df=1.0, tf_idf_min_df=1,\n",
    "                 lsa_components: int = 100, svd_n_iter: int = 5,\n",
    "                 n_top_words: int = 10, ngram_range: tuple = (1, 1),\n",
    "                 random_state: int = -1):\n",
    "        self.import_documents_list = EnglishPreprocessor().preprocess_documents(documents_list)\n",
    "        self.tf_idf_max_df = tf_idf_max_df\n",
    "        self.tf_idf_min_df = tf_idf_min_df\n",
    "        self.lsa_components = lsa_components\n",
    "        self.svd_n_iter = svd_n_iter\n",
    "        self.n_top_words = n_top_words\n",
    "        self.ngram_range = ngram_range\n",
    "        self.random_state = random_state\n",
    "        self.coherence_texts_calculated = False\n",
    "\n",
    "    def run_topics_detection(self):\n",
    "        tfidf_documents = self.TF_IDF()\n",
    "        self.TruncatedSVD(tfidf_documents)\n",
    "        self.topics = self.find_topics()\n",
    "        return self.topics\n",
    "\n",
    "    def transform_documents(self, new_documents_list: list):\n",
    "        return list(map(np.argmax, self.svd_model.transform(\n",
    "            self.tfidf_vectorizer.transform(new_documents_list))))\n",
    "\n",
    "    def import_ready_documents(self, documents_list, texts, dictionary, corpus):\n",
    "        self.import_documents_list = documents_list\n",
    "        self.texts = texts\n",
    "        self.dictionary = dictionary\n",
    "        self.corpus = corpus\n",
    "        self.coherence_texts_calculated = True\n",
    "\n",
    "    def TF_IDF(self):\n",
    "        self.tfidf_vectorizer = TfidfVectorizer(max_df=self.tf_idf_max_df,\n",
    "                                                min_df=self.tf_idf_min_df,\n",
    "                                                ngram_range=self.ngram_range,\n",
    "                                                stop_words='english')\n",
    "        return self.tfidf_vectorizer.fit_transform(self.import_documents_list)\n",
    "        # tfidf_feature_names = self.tfidf_vectorizer.get_feature_names_out()\n",
    "    \n",
    "    def TruncatedSVD(self, tfidf_documents):\n",
    "        if self.random_state != -1:\n",
    "            self.svd_model = TruncatedSVD(n_components=self.lsa_components,\n",
    "                                 n_iter=self.svd_n_iter, random_state=self.random_state)\n",
    "        else:\n",
    "            self.svd_model = TruncatedSVD(n_components=self.lsa_components,\n",
    "                                 n_iter=self.svd_n_iter)\n",
    "        self.svd_model.fit_transform(tfidf_documents)\n",
    "\n",
    "    def find_topics(self):\n",
    "        # Extract the top words for each topic\n",
    "        topics = []\n",
    "        tfidf_feature_names = self.tfidf_vectorizer.get_feature_names_out()\n",
    "        for topic_idx, topic in enumerate(self.svd_model.components_):\n",
    "            top_features_ind = topic.argsort()[:-self.n_top_words - 1:-1]\n",
    "            top_features = [tfidf_feature_names[i] for i in top_features_ind]\n",
    "            topics.append(top_features)\n",
    "        return topics\n",
    "\n",
    "    def calculate_coherence_score(self, recalculate_texts: bool = False, verbose: int = 0):\n",
    "        if not self.coherence_texts_calculated or recalculate_texts:\n",
    "            self.calculate_coherence_texts(verbose=verbose)\n",
    "        # Calculate the coherence score using Gensim\n",
    "        coherence_model = CoherenceModel(topics=self.topics,\n",
    "                                         texts=self.texts,\n",
    "                                         dictionary=self.dictionary,\n",
    "                                         coherence='c_v')\n",
    "        if verbose == 1:\n",
    "            print(f'--Calculating the coherence score')\n",
    "        self.coherence_score = coherence_model.get_coherence()\n",
    "        return self.coherence_score\n",
    "\n",
    "    def calculate_coherence_texts(self, verbose: int = 0):\n",
    "        # Convert the list of top words into a list of lists of words\n",
    "        tfidf_feature_names = set(self.tfidf_vectorizer.get_feature_names_out())\n",
    "        if verbose == 1:\n",
    "            print(f'--Starting forming the texts')\n",
    "        self.texts = [[word for word in doc.lower().split() if (\n",
    "            word in tfidf_feature_names)] for doc in self.import_documents_list]\n",
    "        # Create a Gensim dictionary\n",
    "        if verbose == 1:\n",
    "            print(f'--Creating the Gensim dectionary')\n",
    "        self.dictionary = Dictionary(self.texts)\n",
    "        # Convert the dictionary and the corpus\n",
    "        if verbose == 1:\n",
    "            print(f'--Converting to the corpus')\n",
    "        self.corpus = [dictionary.doc2bow(text) for text in self.texts]\n",
    "        self.coherence_texts_calculated = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "0217a794-c922-4ef7-a8e1-c26eceee0f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Documents for the day 2023-08-19 processed\n",
      "--Documents for the day 2023-08-20 processed\n",
      "--Documents for the day 2023-08-21 processed\n",
      "--Documents for the day 2023-08-22 processed\n",
      "--Documents for the day 2023-08-23 processed\n"
     ]
    }
   ],
   "source": [
    "all_documents_19082308 = get_uctd_documents_between_dates('2023-08-19','2023-08-23',verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01897e9f-36fa-4a54-8081-23c095f889da",
   "metadata": {},
   "source": [
    "First run - preprocessing with filtering out all of the hashtags without exception:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "ef85e329-4b31-47e3-aba8-9f3abb40309d",
   "metadata": {},
   "outputs": [],
   "source": [
    "LSAPipeline = LSAPipelineEnglish(all_documents_19082308, tf_idf_max_df=0.9, tf_idf_min_df=4,\n",
    "                                 lsa_components=5, svd_n_iter=5,\n",
    "                                 n_top_words=10, random_state=47)\n",
    "topics_v1_1 = LSAPipeline.run_topics_detection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "ccf91421-3e68-4dc4-8ba5-08edeaeb7c6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ukraine',\n",
       "  'russia',\n",
       "  'russian',\n",
       "  'war',\n",
       "  'ukrainian',\n",
       "  'putin',\n",
       "  'day',\n",
       "  'amp',\n",
       "  'people',\n",
       "  'state'],\n",
       " ['awesome',\n",
       "  'passive',\n",
       "  'hire',\n",
       "  'website',\n",
       "  'income',\n",
       "  'create',\n",
       "  'term',\n",
       "  'autopilot',\n",
       "  'long',\n",
       "  'developer'],\n",
       " ['russian',\n",
       "  'russia',\n",
       "  'putin',\n",
       "  'ukrainian',\n",
       "  'pariah',\n",
       "  'barbarism',\n",
       "  'limit',\n",
       "  'state',\n",
       "  'defeated',\n",
       "  'crimeabelongstoukraine'],\n",
       " ['russia',\n",
       "  'war',\n",
       "  'putin',\n",
       "  'stop',\n",
       "  'ukraine',\n",
       "  'news',\n",
       "  'latest',\n",
       "  'breaking',\n",
       "  'support',\n",
       "  'state'],\n",
       " ['putin',\n",
       "  'daughter',\n",
       "  'car',\n",
       "  'killed',\n",
       "  'dugin',\n",
       "  'bomb',\n",
       "  'alexander',\n",
       "  'moscow',\n",
       "  'amp',\n",
       "  'ally']]"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_v1_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "238c18bd-caca-4857-a0b5-7cdd0216662d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Score: 0.7628949591395007\n"
     ]
    }
   ],
   "source": [
    "print('Coherence Score:', LSAPipeline.calculate_coherence_score())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9d3fdb-5370-44fe-9dd3-b457e148794a",
   "metadata": {},
   "source": [
    "New hashtag calculation scheme: only hastags at the end of the tweet (not counting the links):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "61356059-f876-4d89-986d-b7996be3218d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnglishPreprocessor:\n",
    "    def preprocess_documents(self, documents_list: list, return_strings: bool = True):\n",
    "        return [self.preprocess(doc, return_string=return_strings) for doc in documents_list]\n",
    "    \n",
    "    # Function to preprocess documents\n",
    "    def preprocess(self, document, return_string: bool = True):\n",
    "        # Tokenize\n",
    "        document = self.remove_links_content(document)\n",
    "        document = self.remove_emails(document)\n",
    "        document = self.remove_multiple_space(document)\n",
    "        document = self.remove_hashtags(document)\n",
    "        document = self.remove_punctuation(document)\n",
    "        document = self.remove_multiple_space(document)\n",
    "        \n",
    "        words = word_tokenize(document.lower())\n",
    "        # Remove stopwords and punctuations\n",
    "        filtered_words = [word for word in words if word.isalnum() and not word in stop_words]\n",
    "        # Lemmatize\n",
    "        lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
    "        if return_string:\n",
    "            return ' '.join(lemmatized_words)\n",
    "        else:\n",
    "            return lemmatized_words\n",
    "    \n",
    "    def remove_links_content(self, text):\n",
    "        text = re.sub(r\"http\\S+\", \"\", text)\n",
    "        return text\n",
    "    \n",
    "    def remove_emails(self, text):\n",
    "        return re.sub('\\S+@\\S*\\s?', '', text)\n",
    "    \n",
    "    def remove_punctuation(self, text):\n",
    "        \"\"\"https://stackoverflow.com/a/37221663\"\"\"\n",
    "        table = str.maketrans({key: None for key in string.punctuation})\n",
    "        return text.translate(table)\n",
    "    \n",
    "    def remove_multiple_space(self, text):\n",
    "        return re.sub(\"\\s\\s+\", \" \", text)\n",
    "\n",
    "    def remove_hashtags(self, text):\n",
    "        old_text = text + '\\n'\n",
    "        new_text = text\n",
    "        while len(new_text) < len(old_text):\n",
    "            old_text = new_text\n",
    "            new_text = re.sub('(?<=[\\s\\n])#\\S+\\s*$', '', new_text)\n",
    "        return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "a784aebb-dc18-45a9-b01b-94b78581840e",
   "metadata": {},
   "outputs": [],
   "source": [
    "LSAPipeline = LSAPipelineEnglish(all_documents_19082308, tf_idf_max_df=0.9, tf_idf_min_df=4,\n",
    "                                 lsa_components=5, svd_n_iter=5,\n",
    "                                 n_top_words=10, random_state=47)\n",
    "topics_v2 = LSAPipeline.run_topics_detection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "4cb2f1cd-f439-4407-8757-b249b1fd7ba3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ukraine',\n",
       "  'russia',\n",
       "  'russian',\n",
       "  'war',\n",
       "  'putin',\n",
       "  'ukrainian',\n",
       "  'day',\n",
       "  'amp',\n",
       "  'people',\n",
       "  'support'],\n",
       " ['russian',\n",
       "  'putin',\n",
       "  'ukrainian',\n",
       "  'russia',\n",
       "  'pariah',\n",
       "  'barbarism',\n",
       "  'slavaukraine',\n",
       "  'limit',\n",
       "  'crimeabelongstoukraine',\n",
       "  'defeated'],\n",
       " ['putin',\n",
       "  'russia',\n",
       "  'war',\n",
       "  'fuck',\n",
       "  'daughter',\n",
       "  'dugin',\n",
       "  'car',\n",
       "  'stop',\n",
       "  'news',\n",
       "  'ally'],\n",
       " ['putin',\n",
       "  'daughter',\n",
       "  'car',\n",
       "  'dugin',\n",
       "  'killed',\n",
       "  'moscow',\n",
       "  'alexander',\n",
       "  'bomb',\n",
       "  'fuck',\n",
       "  'dugina'],\n",
       " ['day',\n",
       "  'flag',\n",
       "  'happy',\n",
       "  'independence',\n",
       "  'putin',\n",
       "  'national',\n",
       "  'ukrainian',\n",
       "  'russia',\n",
       "  'fuck',\n",
       "  'state']]"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "90acf711-9b91-4cde-b50b-1f01184f6682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Score: 0.6348205406793523\n"
     ]
    }
   ],
   "source": [
    "print('Coherence Score:', LSAPipeline.calculate_coherence_score())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67122c1b-3d15-438c-a97b-a474b4c2d15a",
   "metadata": {},
   "source": [
    "# bigrams\n",
    "\n",
    "Model for the only bigrams (2-word ingrams):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "4c26089d-c872-4e2e-b8d7-55aba4dd6e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "LSAPipeline = LSAPipelineEnglish(all_documents_19082308, tf_idf_max_df=0.9, tf_idf_min_df=4,\n",
    "                                 lsa_components=5, svd_n_iter=5,\n",
    "                                 n_top_words=10, ngram_range=(2,2),\n",
    "                                 random_state=47)\n",
    "topics_v3 = LSAPipeline.run_topics_detection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "a2d88ca1-4f0c-4f83-8c32-a9476a84e4c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['long term',\n",
       "  'term passive',\n",
       "  'website long',\n",
       "  'hire awesome',\n",
       "  'passive income',\n",
       "  'create autopilot',\n",
       "  'awesome developer',\n",
       "  'developer create',\n",
       "  'clickbank website',\n",
       "  'autopilot clickbank'],\n",
       " ['putin russian',\n",
       "  'possible russia',\n",
       "  'defeated ukrainian',\n",
       "  'barbarism limit',\n",
       "  'limit peace',\n",
       "  'russian barbarism',\n",
       "  'peace possible',\n",
       "  'russia pariah',\n",
       "  'pariah state',\n",
       "  'russia defeated'],\n",
       " ['looking follow',\n",
       "  'trump looking',\n",
       "  'common sense',\n",
       "  'armed force',\n",
       "  'flag day',\n",
       "  'analysis stats',\n",
       "  'air defense',\n",
       "  'gosloto 536',\n",
       "  '536 result',\n",
       "  'follow help'],\n",
       " ['standwithukraine playing',\n",
       "  'rock radio',\n",
       "  'arvada rock',\n",
       "  'playing arvada',\n",
       "  'playing metallica',\n",
       "  'playing rush',\n",
       "  'love video',\n",
       "  'van halen',\n",
       "  'playing van',\n",
       "  'playing aerosmith'],\n",
       " ['fab topbananaantiques',\n",
       "  'topbananamall fab',\n",
       "  'topbananaantiques celebritymasterchef',\n",
       "  '9k gold',\n",
       "  'sterling silver',\n",
       "  'youtube fab',\n",
       "  'silver plated',\n",
       "  'topbananaantiques rosewednesday',\n",
       "  'yellow gold',\n",
       "  '9k yellow']]"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "d814ff44-126f-4cb5-a577-932ad5484e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "LSAPipeline = LSAPipelineEnglish(all_documents_19082308, tf_idf_max_df=0.9, tf_idf_min_df=4,\n",
    "                                 lsa_components=20, svd_n_iter=5,\n",
    "                                 n_top_words=10, ngram_range=(2,2),\n",
    "                                 random_state=47)\n",
    "topics_v4 = LSAPipeline.run_topics_detection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "a59601b4-7655-4375-8bcb-09a3152f88f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['long term',\n",
       "  'term passive',\n",
       "  'website long',\n",
       "  'hire awesome',\n",
       "  'passive income',\n",
       "  'create autopilot',\n",
       "  'awesome developer',\n",
       "  'developer create',\n",
       "  'clickbank website',\n",
       "  'autopilot clickbank'],\n",
       " ['putin russian',\n",
       "  'possible russia',\n",
       "  'defeated ukrainian',\n",
       "  'russian barbarism',\n",
       "  'limit peace',\n",
       "  'peace possible',\n",
       "  'barbarism limit',\n",
       "  'russia pariah',\n",
       "  'pariah state',\n",
       "  'russia defeated'],\n",
       " ['looking follow',\n",
       "  'trump looking',\n",
       "  'common sense',\n",
       "  'look like',\n",
       "  'need vote',\n",
       "  'follow help',\n",
       "  'fake news',\n",
       "  'dont understand',\n",
       "  'brain dead',\n",
       "  'know fight'],\n",
       " ['standwithukraine playing',\n",
       "  'arvada rock',\n",
       "  'playing arvada',\n",
       "  'rock radio',\n",
       "  'playing metallica',\n",
       "  'playing rush',\n",
       "  'love video',\n",
       "  'van halen',\n",
       "  'playing van',\n",
       "  'playing aerosmith'],\n",
       " ['fab topbananaantiques',\n",
       "  'topbananamall fab',\n",
       "  'topbananaantiques celebritymasterchef',\n",
       "  '9k gold',\n",
       "  'sterling silver',\n",
       "  'youtube fab',\n",
       "  'silver plated',\n",
       "  'topbananaantiques rosewednesday',\n",
       "  'yellow gold',\n",
       "  '9k yellow'],\n",
       " ['russia ukraine',\n",
       "  'war russia',\n",
       "  'breaking news',\n",
       "  'news latest',\n",
       "  'latest war',\n",
       "  'today breaking',\n",
       "  'ukraine war',\n",
       "  'power plant',\n",
       "  'proxy war',\n",
       "  'nuclear power'],\n",
       " ['analysis stats',\n",
       "  'gosloto 536',\n",
       "  '536 result',\n",
       "  'gosloto 645',\n",
       "  '645 result',\n",
       "  'gosloto 749',\n",
       "  '749 result',\n",
       "  'terrorist state',\n",
       "  'russia terrorist',\n",
       "  'people ukraine'],\n",
       " ['analysis article',\n",
       "  'report viewed',\n",
       "  'viewed im',\n",
       "  'content analysis',\n",
       "  'complete report',\n",
       "  'article score',\n",
       "  'im bot',\n",
       "  '69100 complete',\n",
       "  'score 69100',\n",
       "  'score 79100'],\n",
       " ['putin war',\n",
       "  'funding putin',\n",
       "  'stop funding',\n",
       "  'citi hsbc',\n",
       "  'hsbc jpmorgan',\n",
       "  'sign petition',\n",
       "  'jpmorgan chase',\n",
       "  'war sign',\n",
       "  'chase crédit',\n",
       "  'crédit agricole'],\n",
       " ['support ukraine',\n",
       "  'stop war',\n",
       "  'ukraine stop',\n",
       "  'kyiv today',\n",
       "  'missile attack',\n",
       "  'eu say',\n",
       "  'civilian kyiv',\n",
       "  'people thought',\n",
       "  'stand resolute',\n",
       "  'partner stand'],\n",
       " ['independence day',\n",
       "  'flag day',\n",
       "  'happy independence',\n",
       "  'day ukraine',\n",
       "  'national flag',\n",
       "  'ukraine independence',\n",
       "  'happy national',\n",
       "  'ukrainian independence',\n",
       "  'happy flag',\n",
       "  'look like'],\n",
       " ['power plant',\n",
       "  'nuclear power',\n",
       "  'look like',\n",
       "  'zaporizhzhia nuclear',\n",
       "  'nuclear plant',\n",
       "  'armed force',\n",
       "  'largest nuclear',\n",
       "  'zaporozhye nuclear',\n",
       "  'shelling nuclear',\n",
       "  'plant ukraine'],\n",
       " ['look like',\n",
       "  'air defense',\n",
       "  'car bombing',\n",
       "  'gon na',\n",
       "  'war ukraine',\n",
       "  'like russian',\n",
       "  'make look',\n",
       "  'russian air',\n",
       "  'air defence',\n",
       "  'car bomb'],\n",
       " ['flag day',\n",
       "  'national flag',\n",
       "  'happy national',\n",
       "  'happy flag',\n",
       "  'ukrainian flag',\n",
       "  'day ukraine',\n",
       "  'day national',\n",
       "  'ukraine flag',\n",
       "  'flag ukraine',\n",
       "  'happy ukrainian'],\n",
       " ['armed force',\n",
       "  'air defense',\n",
       "  'force ukraine',\n",
       "  'russian air',\n",
       "  'ukrainian armed',\n",
       "  'general staff',\n",
       "  'russian armed',\n",
       "  'staff armed',\n",
       "  'defense working',\n",
       "  'black sea'],\n",
       " ['air defense',\n",
       "  'russian air',\n",
       "  'good morning',\n",
       "  'defense working',\n",
       "  'defense active',\n",
       "  'united state',\n",
       "  'defense worked',\n",
       "  'save defender',\n",
       "  'irist air',\n",
       "  'power plant'],\n",
       " ['mur fuck',\n",
       "  'fuck lilek',\n",
       "  'putin maltese',\n",
       "  'innifsek putin',\n",
       "  'lilek innifsek',\n",
       "  'good morning',\n",
       "  'check new',\n",
       "  'uploaded peace',\n",
       "  'maltese ukraine',\n",
       "  'maltese putinwarcrimes'],\n",
       " ['check new',\n",
       "  'uploaded peace',\n",
       "  'print uploaded',\n",
       "  'digital art',\n",
       "  'art uploaded',\n",
       "  'new digital',\n",
       "  'war crime',\n",
       "  'new drawing',\n",
       "  'drawing uploaded',\n",
       "  'new framed'],\n",
       " ['good morning',\n",
       "  'missile attack',\n",
       "  'kyiv today',\n",
       "  'attack civilian',\n",
       "  'say people',\n",
       "  'people thought',\n",
       "  'civilian kyiv',\n",
       "  'eu say',\n",
       "  'russia target',\n",
       "  'thought russia'],\n",
       " ['good morning',\n",
       "  'terrorist state',\n",
       "  'russia terrorist',\n",
       "  'people ukraine',\n",
       "  'fuck putin',\n",
       "  'help people',\n",
       "  'whats happening',\n",
       "  'follow help',\n",
       "  'article link',\n",
       "  'car bomb']]"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_v4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "2846260a-aaf6-4015-83e8-6a9792059db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Starting forming the texts\n",
      "--Creating the Gensim dectionary\n",
      "--Converting to the corpus\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "unable to interpret topic as either a list of tokens or a list of ids",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[256], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCoherence Score:\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[43mLSAPipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalculate_coherence_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[249], line 53\u001b[0m, in \u001b[0;36mLSAPipelineEnglish.calculate_coherence_score\u001b[0;34m(self, recalculate_texts, verbose)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalculate_coherence_texts(verbose\u001b[38;5;241m=\u001b[39mverbose)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Calculate the coherence score using Gensim\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m coherence_model \u001b[38;5;241m=\u001b[39m \u001b[43mCoherenceModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtopics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtopics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mtexts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mdictionary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdictionary\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mcoherence\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mc_v\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--Calculating the coherence score\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/UCU/ucu_env/lib/python3.9/site-packages/gensim/models/coherencemodel.py:214\u001b[0m, in \u001b[0;36mCoherenceModel.__init__\u001b[0;34m(self, model, topics, texts, corpus, dictionary, window_size, keyed_vectors, coherence, topn, processes)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accumulator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_topics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 214\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtopics\u001b[49m \u001b[38;5;241m=\u001b[39m topics\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocesses \u001b[38;5;241m=\u001b[39m processes \u001b[38;5;28;01mif\u001b[39;00m processes \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m1\u001b[39m, mp\u001b[38;5;241m.\u001b[39mcpu_count() \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/UCU/ucu_env/lib/python3.9/site-packages/gensim/models/coherencemodel.py:429\u001b[0m, in \u001b[0;36mCoherenceModel.topics\u001b[0;34m(self, topics)\u001b[0m\n\u001b[1;32m    427\u001b[0m new_topics \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m topic \u001b[38;5;129;01min\u001b[39;00m topics:\n\u001b[0;32m--> 429\u001b[0m     topic_token_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_elements_are_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtopic\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    430\u001b[0m     new_topics\u001b[38;5;241m.\u001b[39mappend(topic_token_ids)\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/UCU/ucu_env/lib/python3.9/site-packages/gensim/models/coherencemodel.py:453\u001b[0m, in \u001b[0;36mCoherenceModel._ensure_elements_are_ids\u001b[0;34m(self, topic)\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(ids_from_ids)\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 453\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munable to interpret topic as either a list of tokens or a list of ids\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: unable to interpret topic as either a list of tokens or a list of ids"
     ]
    }
   ],
   "source": [
    "print('Coherence Score:', LSAPipeline.calculate_coherence_score(verbose=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "df45e3b2-e7bf-497d-8607-2714e59376f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = LSAPipeline.texts\n",
    "dictionary = LSAPipeline.dictionary\n",
    "corpus = LSAPipeline.corpus\n",
    "tfidf_feature_names = set(LSAPipeline.tfidf_vectorizer.get_feature_names_out())\n",
    "preprocessed_documents = LSAPipeline.import_documents_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "52e6b8fa-eff8-4f69-993f-bcd668e5713a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['000 gazprom', '000 people', '000000 month', ...,\n",
       "       'ⳅᘖꮞ linkwhapsapp', 'ⳋiⳋ ⳅᘖꮞ', 'ⳋꚨӏ ƽƽƽ'], dtype=object)"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "cdb187f6-1aef-4dcb-9feb-ed200b839737",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "6520c084-e66a-4c0e-b247-5a9291689dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_ngrams(document, n_g):\n",
    "    return [' '.join(ngram) for ngram in ngrams(document.lower().split(), n_g)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "e0b5e3f4-1e6a-40ba-b56d-abf19d258748",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dear vaccine',\n",
       " 'vaccine advocate',\n",
       " 'advocate take',\n",
       " 'take covid19',\n",
       " 'covid19 mrna',\n",
       " 'mrna shot',\n",
       " 'shot booster',\n",
       " 'booster know',\n",
       " 'know ourworldindata',\n",
       " 'ourworldindata data',\n",
       " 'data show',\n",
       " 'show offer',\n",
       " 'offer zero',\n",
       " 'zero protection',\n",
       " 'protection actually',\n",
       " 'actually accelerates',\n",
       " 'accelerates death',\n",
       " 'death vaccinated',\n",
       " 'vaccinated regard']"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_ngrams(preprocessed_documents[0], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "3af105d1-52a5-4eba-a91d-5d6f66794bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of ngrams into a list of lists of words\n",
    "texts_v2 = [tokenize_ngrams(doc, 2) for doc in preprocessed_documents]\n",
    "\n",
    "# Create a Gensim dictionary\n",
    "dictionary_v2 = Dictionary(texts_v2)\n",
    "\n",
    "# Convert the dictionary and the corpus\n",
    "corpus_v2 = [dictionary_v2.doc2bow(text) for text in texts_v2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "c6648aba-325f-41fd-ab8f-605d536a46c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Score: 0.5825185780434505\n"
     ]
    }
   ],
   "source": [
    "# Calculate the coherence score using Gensim\n",
    "coherence_model = CoherenceModel(topics=topics_v4,\n",
    "                                 texts=texts_v2,\n",
    "                                 dictionary=dictionary_v2, coherence='c_v')\n",
    "coherence_score = coherence_model.get_coherence()\n",
    "\n",
    "print('Coherence Score:', coherence_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "91d5f116-9266-422a-ae3a-65578c489afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Score: 0.5825185780434505\n"
     ]
    }
   ],
   "source": [
    "# Calculate the coherence score using Gensim\n",
    "coherence_model = CoherenceModel(topics=topics_v4,\n",
    "                                 texts=texts_v2,\n",
    "                                 dictionary=dictionary_v2,\n",
    "                                 corpus=corpus_v2,\n",
    "                                 coherence='c_v')\n",
    "coherence_score = coherence_model.get_coherence()\n",
    "\n",
    "print('Coherence Score:', coherence_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "8959c86c-3437-4468-b607-5f6556873ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of ngrams into a list of lists of words\n",
    "texts_v3 = [[ngram for ngram in tokenize_ngrams(\n",
    "    doc, 2) if ngram in tfidf_feature_names] for doc in preprocessed_documents]\n",
    "\n",
    "# Create a Gensim dictionary\n",
    "dictionary_v3 = Dictionary(texts_v3)\n",
    "\n",
    "# Convert the dictionary and the corpus\n",
    "corpus_v3 = [dictionary_v3.doc2bow(text) for text in texts_v3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "fe7dd877-c41f-4f8e-931b-b0cfe14f47d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Score: 0.5825185780434505\n"
     ]
    }
   ],
   "source": [
    "# Calculate the coherence score using Gensim\n",
    "coherence_model = CoherenceModel(topics=topics_v4,\n",
    "                                 texts=texts_v3,\n",
    "                                 dictionary=dictionary_v3,\n",
    "                                 corpus=corpus_v3,\n",
    "                                 coherence='c_v')\n",
    "coherence_score = coherence_model.get_coherence()\n",
    "\n",
    "print('Coherence Score:', coherence_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7466d4-d0d4-466f-8c40-b5b4e9444596",
   "metadata": {},
   "source": [
    "# 1- and 2-grams\n",
    "\n",
    "Model that uses both 1- and 2-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "2558dd1e-1d4f-4f70-a698-3845321b7813",
   "metadata": {},
   "outputs": [],
   "source": [
    "LSAPipeline = LSAPipelineEnglish(all_documents_19082308, tf_idf_max_df=0.9, tf_idf_min_df=4,\n",
    "                                 lsa_components=5, svd_n_iter=5,\n",
    "                                 n_top_words=10, ngram_range=(1,2),\n",
    "                                 random_state=47)\n",
    "topics_v5 = LSAPipeline.run_topics_detection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "fdf4700b-2c92-4657-ac9d-7e00f2451449",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ukraine',\n",
       "  'russia',\n",
       "  'russian',\n",
       "  'war',\n",
       "  'ukrainian',\n",
       "  'putin',\n",
       "  'day',\n",
       "  'amp',\n",
       "  'state',\n",
       "  'people'],\n",
       " ['passive',\n",
       "  'awesome',\n",
       "  'hire',\n",
       "  'long term',\n",
       "  'income',\n",
       "  'term',\n",
       "  'term passive',\n",
       "  'autopilot',\n",
       "  'create',\n",
       "  'hire awesome'],\n",
       " ['possible russia',\n",
       "  'barbarism limit',\n",
       "  'russian barbarism',\n",
       "  'peace possible',\n",
       "  'limit peace',\n",
       "  'russia pariah',\n",
       "  'pariah state',\n",
       "  'defeated ukrainian',\n",
       "  'putin russian',\n",
       "  'pariah'],\n",
       " ['looking follow',\n",
       "  'trump looking',\n",
       "  'follow',\n",
       "  'trump',\n",
       "  'looking',\n",
       "  'amp',\n",
       "  'russian',\n",
       "  'standwithukraine',\n",
       "  'video',\n",
       "  'playing'],\n",
       " ['russian',\n",
       "  'standwithukraine',\n",
       "  'video',\n",
       "  'playing',\n",
       "  'standwithukraine playing',\n",
       "  'amp',\n",
       "  'ukrainian',\n",
       "  'day',\n",
       "  'daughter',\n",
       "  'like']]"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_v5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "5cd7c00e-c30b-4f7b-8716-15b0f95cd836",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_feature_names = set(LSAPipeline.tfidf_vectorizer.get_feature_names_out())\n",
    "preprocessed_documents = LSAPipeline.import_documents_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "1539c443-e93a-4769-b3f6-151069e52586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of ngrams into a list of lists of words\n",
    "texts_v4 = [[word for word in doc.lower().split() if (\n",
    "    word in tfidf_feature_names)] + tokenize_ngrams(doc, 2) for doc in preprocessed_documents]\n",
    "\n",
    "# Create a Gensim dictionary\n",
    "dictionary_v4 = Dictionary(texts_v4)\n",
    "\n",
    "# Convert the dictionary and the corpus\n",
    "corpus_v4 = [dictionary_v4.doc2bow(text) for text in texts_v4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "bd4ee8e1-fe36-4968-b6c9-d853da1d1d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Score: 0.6268690286886711\n"
     ]
    }
   ],
   "source": [
    "# Calculate the coherence score using Gensim\n",
    "coherence_model = CoherenceModel(topics=topics_v5,\n",
    "                                 texts=texts_v4,\n",
    "                                 dictionary=dictionary_v4,\n",
    "                                 corpus=corpus_v4,\n",
    "                                 coherence='c_v')\n",
    "coherence_score = coherence_model.get_coherence()\n",
    "\n",
    "print('Coherence Score:', coherence_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "69626a4c-7bbe-4506-8eb7-79b61a6d6341",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dear vaccine advocate take covid19 mrna shot booster know ourworldindata data show offer zero protection actually accelerates death vaccinated regard',\n",
       " 'animal shelter dog cat need help raising fund food animal paypal',\n",
       " 'welcome shelter located ukraine kyiv shelter need help raising fund food animal paypal',\n",
       " 'good news may missed first wfp shipment wheat operation left 17 aug key milestone assist vulnerable amp affected global crisis',\n",
       " 'opinion ukraine war also fought language cnn']"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_documents[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "97e77420-3c67-4a2c-bfe4-df6c4d3f9a6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Now That President Trump has Left Office, Will The Whole Toad News Service Shut Their Doors? | by Don Feazelle | MuddyUm | Jan, 2021 | Medium - via @pensignal  https://t.co/BLJ8xEd6Op #Humor #Satire #WholeToadNewsService #Trump #Biden #PoliticalInsanity'"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_documents[49]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "76bc62e8-dd7e-42b7-b95e-ee18d1734c4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 3]"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(np.argmax, LSAPipeline.svd_model.transform(LSAPipeline.tfidf_vectorizer.transform(preprocessed_documents[40:50]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ea62f7-bd17-4a20-98db-8c4b59c251bb",
   "metadata": {},
   "source": [
    "## 1- and 2-grams: optimal topics number\n",
    "\n",
    "Model that uses both 1- and 2-grams for the different topics numbers for the data for September 2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "80b96e23-00c7-455f-90a3-bb1575e6e855",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Documents for the day 2023-09-01 processed\n",
      "--Documents for the day 2023-09-02 processed\n",
      "--Documents for the day 2023-09-03 processed\n",
      "--Documents for the day 2023-09-04 processed\n",
      "--Documents for the day 2023-09-05 processed\n",
      "--Documents for the day 2023-09-06 processed\n",
      "--Documents for the day 2023-09-07 processed\n",
      "--Documents for the day 2023-09-08 processed\n",
      "--Documents for the day 2023-09-09 processed\n",
      "--Documents for the day 2023-09-10 processed\n",
      "--Documents for the day 2023-09-11 processed\n",
      "--Documents for the day 2023-09-12 processed\n",
      "--Documents for the day 2023-09-13 processed\n",
      "--Documents for the day 2023-09-14 processed\n",
      "--Documents for the day 2023-09-15 processed\n",
      "--Documents for the day 2023-09-16 processed\n",
      "--Documents for the day 2023-09-17 processed\n",
      "--Documents for the day 2023-09-18 processed\n",
      "--Documents for the day 2023-09-19 processed\n",
      "--Documents for the day 2023-09-20 processed\n",
      "--Documents for the day 2023-09-21 processed\n",
      "--Documents for the day 2023-09-22 processed\n",
      "--Documents for the day 2023-09-23 processed\n",
      "--Documents for the day 2023-09-24 processed\n",
      "--Documents for the day 2023-09-25 processed\n",
      "--Documents for the day 2023-09-26 processed\n",
      "--Documents for the day 2023-09-27 processed\n",
      "--Documents for the day 2023-09-28 processed\n",
      "--Documents for the day 2023-09-29 processed\n",
      "--Documents for the day 2023-09-30 processed\n"
     ]
    }
   ],
   "source": [
    "all_documents_092022 = get_uctd_documents_between_dates('2023-09-01','2023-09-30',verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a01cf2f-2925-4679-b9d9-77a29839ca2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Topic number 2 coherence score: 0.6710605575748958\n",
      "--Topic number 3 coherence score: 0.7230837885786433\n"
     ]
    }
   ],
   "source": [
    "topic_numbers_12grams = list(range(2,15)) + [20,30,40,50,100,150,200]\n",
    "coherences_12grams = []\n",
    "predicted_topics_counts_12grams = []\n",
    "for topic_number in topic_numbers_12grams:\n",
    "    LSAPipeline = LSAPipelineEnglish(all_documents_19082308, tf_idf_max_df=0.9, tf_idf_min_df=4,\n",
    "                                 lsa_components=topic_number, svd_n_iter=5,\n",
    "                                 n_top_words=10, ngram_range=(1,2),\n",
    "                                 random_state=47)\n",
    "    topics_12grams = LSAPipeline.run_topics_detection()\n",
    "    tfidf_feature_names = set(LSAPipeline.tfidf_vectorizer.get_feature_names_out())\n",
    "    # Calculate the coherence score using Gensim\n",
    "    coherence_model = CoherenceModel(topics=topics_12grams,\n",
    "                                     texts=texts_v4,\n",
    "                                     dictionary=dictionary_v4,\n",
    "                                     corpus=corpus_v4,\n",
    "                                     coherence='c_v')\n",
    "    coherence_score = coherence_model.get_coherence()\n",
    "    predicted_topics = LSAPipeline.transform_documents(LSAPipeline.import_documents_list)\n",
    "    predicted_topics_count = [predicted_topics.count(x) for x in np.sort(np.unique(predicted_topics))]\n",
    "    \n",
    "    print(f'--Topic number {topic_number} coherence score: {coherence_score}')\n",
    "    coherences_12grams.append(coherence_score)\n",
    "    predicted_topics_counts_12grams.append(predicted_topics_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f620ded1-9fa2-42a8-a50c-422600ca1919",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('coherences_12grams.json','w') as json_file:\n",
    "    json.dump({'coherences': coherences_12grams,\n",
    "               'predicted_topics': predicted_topics_counts_12grams}, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33ff661-9f46-4c7a-8f46-9a46eb0410a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(coherences_12grams, topic_numbers_12grams)\n",
    "plt.title('Coherence for different n_topics | 12grams model')\n",
    "plt.xlabel('Coherence (c_v) score')\n",
    "plt.ylabel('Topics number')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f46451-3d92-43ba-a916-4aecb48f1a4d",
   "metadata": {},
   "source": [
    "# 1- and 2-grams: limited preprocessing\n",
    "\n",
    "Model that uses both 1- and 2-grams with the decreased preprocessing.\n",
    "\n",
    "With no stopwords removal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "3a728f39-6d02-418b-8875-2771e54f9f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnglishPreprocessor:\n",
    "    def preprocess_documents(self, documents_list: list, return_strings: bool = True):\n",
    "        return [self.preprocess(doc, return_string=return_strings) for doc in documents_list]\n",
    "    \n",
    "    # Function to preprocess documents\n",
    "    def preprocess(self, document, return_string: bool = True):\n",
    "        # Tokenize\n",
    "        document = self.remove_links_content(document)\n",
    "        document = self.remove_emails(document)\n",
    "        document = self.remove_multiple_space(document)\n",
    "        document = self.remove_hashtags(document)\n",
    "        document = self.remove_punctuation(document)\n",
    "        document = self.remove_multiple_space(document)\n",
    "        \n",
    "        words = word_tokenize(document.lower())\n",
    "        # Remove stopwords and punctuations\n",
    "        # filtered_words = [word for word in words if word.isalnum() and not word in stop_words]\n",
    "        filtered_words = [word for word in words if word.isalnum()]\n",
    "        # Lemmatize\n",
    "        lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
    "        if return_string:\n",
    "            return ' '.join(lemmatized_words)\n",
    "        else:\n",
    "            return lemmatized_words\n",
    "    \n",
    "    def remove_links_content(self, text):\n",
    "        text = re.sub(r\"http\\S+\", \"\", text)\n",
    "        return text\n",
    "    \n",
    "    def remove_emails(self, text):\n",
    "        return re.sub('\\S+@\\S*\\s?', '', text)\n",
    "    \n",
    "    def remove_punctuation(self, text):\n",
    "        \"\"\"https://stackoverflow.com/a/37221663\"\"\"\n",
    "        table = str.maketrans({key: None for key in string.punctuation})\n",
    "        return text.translate(table)\n",
    "    \n",
    "    def remove_multiple_space(self, text):\n",
    "        return re.sub(\"\\s\\s+\", \" \", text)\n",
    "\n",
    "    def remove_hashtags(self, text):\n",
    "        old_text = text + '\\n'\n",
    "        new_text = text\n",
    "        while len(new_text) < len(old_text):\n",
    "            old_text = new_text\n",
    "            new_text = re.sub('(?<=[\\s\\n])#\\S+\\s*$', '', new_text)\n",
    "        return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "9f11da0d-a3a2-42f2-a0ad-ea56d891e742",
   "metadata": {},
   "outputs": [],
   "source": [
    "LSAPipeline = LSAPipelineEnglish(all_documents_19082308, tf_idf_max_df=0.9, tf_idf_min_df=4,\n",
    "                                 lsa_components=5, svd_n_iter=5,\n",
    "                                 n_top_words=10, ngram_range=(1,2),\n",
    "                                 random_state=47)\n",
    "topics_v6 = LSAPipeline.run_topics_detection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "fd58e259-e9bc-47eb-94d4-47290ac08821",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ukraine',\n",
       "  'russia',\n",
       "  'russian',\n",
       "  'war',\n",
       "  'putin',\n",
       "  'ukrainian',\n",
       "  'amp',\n",
       "  'day',\n",
       "  'wa',\n",
       "  'ha'],\n",
       " ['passive',\n",
       "  'awesome',\n",
       "  'hire',\n",
       "  'affiliate',\n",
       "  'income',\n",
       "  'term',\n",
       "  'long term',\n",
       "  'term passive',\n",
       "  'autopilot',\n",
       "  'website'],\n",
       " ['possible russia',\n",
       "  'barbarism limit',\n",
       "  'slavaukraine russia',\n",
       "  'limit peace',\n",
       "  'peace possible',\n",
       "  'russian barbarism',\n",
       "  'russia pariah',\n",
       "  'defeated ukrainian',\n",
       "  'pariah state',\n",
       "  'barbarism'],\n",
       " ['putin',\n",
       "  'russia',\n",
       "  'fuck',\n",
       "  'trump',\n",
       "  'russia putin',\n",
       "  'seanhannity tuckercarlson',\n",
       "  'karilake seanhannity',\n",
       "  'donaldjtrumpjr repmtg',\n",
       "  'kimkbaltimore karilake',\n",
       "  'kimkbaltimore'],\n",
       " ['ukraine',\n",
       "  'russia',\n",
       "  'war',\n",
       "  'news',\n",
       "  'russia ukraine',\n",
       "  'war russia',\n",
       "  'news latest',\n",
       "  'today breaking',\n",
       "  'latest war',\n",
       "  'breaking news']]"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_v6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "e0a0981b-cf13-4329-98ec-b4a3442737af",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_feature_names = set(LSAPipeline.tfidf_vectorizer.get_feature_names_out())\n",
    "preprocessed_documents = LSAPipeline.import_documents_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "5b28182e-f82e-4457-90c1-c67170981d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of ngrams into a list of lists of words\n",
    "texts_v6 = [[word for word in doc.lower().split() if (\n",
    "    word in tfidf_feature_names)] + tokenize_ngrams(doc, 2) for doc in preprocessed_documents]\n",
    "\n",
    "# Create a Gensim dictionary\n",
    "dictionary_v6 = Dictionary(texts_v6)\n",
    "\n",
    "# Convert the dictionary and the corpus\n",
    "corpus_v6 = [dictionary_v6.doc2bow(text) for text in texts_v6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "bed167e8-a8a4-48a8-8324-e31999a545d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Score: 0.6655030311944319\n"
     ]
    }
   ],
   "source": [
    "# Calculate the coherence score using Gensim\n",
    "coherence_model = CoherenceModel(topics=topics_v6,\n",
    "                                 texts=texts_v6,\n",
    "                                 dictionary=dictionary_v6,\n",
    "                                 corpus=corpus_v6,\n",
    "                                 coherence='c_v')\n",
    "coherence_score = coherence_model.get_coherence()\n",
    "\n",
    "print('Coherence Score:', coherence_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba27edb8-d521-43de-ba32-f731d1bca20b",
   "metadata": {},
   "source": [
    "With no lemmatization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "d3992397-690a-454f-9512-9b523a74209e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnglishPreprocessor:\n",
    "    def preprocess_documents(self, documents_list: list, return_strings: bool = True):\n",
    "        return [self.preprocess(doc, return_string=return_strings) for doc in documents_list]\n",
    "    \n",
    "    # Function to preprocess documents\n",
    "    def preprocess(self, document, return_string: bool = True):\n",
    "        # Tokenize\n",
    "        document = self.remove_links_content(document)\n",
    "        document = self.remove_emails(document)\n",
    "        document = self.remove_multiple_space(document)\n",
    "        document = self.remove_hashtags(document)\n",
    "        document = self.remove_punctuation(document)\n",
    "        document = self.remove_multiple_space(document)\n",
    "        \n",
    "        words = word_tokenize(document.lower())\n",
    "        # Remove stopwords and punctuations\n",
    "        filtered_words = [word for word in words if word.isalnum() and not word in stop_words]\n",
    "        if return_string:\n",
    "            return ' '.join(filtered_words)\n",
    "        else:\n",
    "            return filtered_words\n",
    "    \n",
    "    def remove_links_content(self, text):\n",
    "        text = re.sub(r\"http\\S+\", \"\", text)\n",
    "        return text\n",
    "    \n",
    "    def remove_emails(self, text):\n",
    "        return re.sub('\\S+@\\S*\\s?', '', text)\n",
    "    \n",
    "    def remove_punctuation(self, text):\n",
    "        \"\"\"https://stackoverflow.com/a/37221663\"\"\"\n",
    "        table = str.maketrans({key: None for key in string.punctuation})\n",
    "        return text.translate(table)\n",
    "    \n",
    "    def remove_multiple_space(self, text):\n",
    "        return re.sub(\"\\s\\s+\", \" \", text)\n",
    "\n",
    "    def remove_hashtags(self, text):\n",
    "        old_text = text + '\\n'\n",
    "        new_text = text\n",
    "        while len(new_text) < len(old_text):\n",
    "            old_text = new_text\n",
    "            new_text = re.sub('(?<=[\\s\\n])#\\S+\\s*$', '', new_text)\n",
    "        return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "684d5a22-bcc2-473f-8529-1922cde80721",
   "metadata": {},
   "outputs": [],
   "source": [
    "LSAPipeline = LSAPipelineEnglish(all_documents_19082308, tf_idf_max_df=0.9, tf_idf_min_df=4,\n",
    "                                 lsa_components=5, svd_n_iter=5,\n",
    "                                 n_top_words=10, ngram_range=(1,2),\n",
    "                                 random_state=47)\n",
    "topics_v6 = LSAPipeline.run_topics_detection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "664d135d-1db9-48b4-b95e-70519f68c473",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ukraine',\n",
       "  'russia',\n",
       "  'war',\n",
       "  'russian',\n",
       "  'putin',\n",
       "  'russians',\n",
       "  'ukrainian',\n",
       "  'amp',\n",
       "  'day',\n",
       "  'people'],\n",
       " ['passive',\n",
       "  'awesome',\n",
       "  'ukraine',\n",
       "  'hire',\n",
       "  'term',\n",
       "  'affiliate',\n",
       "  'income',\n",
       "  'long term',\n",
       "  'term passive',\n",
       "  'autopilot'],\n",
       " ['russians',\n",
       "  'putin russians',\n",
       "  'possible russia',\n",
       "  'defeated ukrainians',\n",
       "  'limits peace',\n",
       "  'barbarism limits',\n",
       "  'russians barbarism',\n",
       "  'peace possible',\n",
       "  'slavaukraine russia',\n",
       "  'russia pariah'],\n",
       " ['putin',\n",
       "  'russia putin',\n",
       "  'trump',\n",
       "  'kimkbaltimore karilake',\n",
       "  'seanhannity tuckercarlson',\n",
       "  'kimkbaltimore',\n",
       "  'tuckercarlson tedcruz',\n",
       "  'donaldjtrumpjr repmtg',\n",
       "  'karilake seanhannity',\n",
       "  'seanhannity'],\n",
       " ['war',\n",
       "  'ukraine',\n",
       "  'russia',\n",
       "  'news',\n",
       "  'russia ukraine',\n",
       "  'war russia',\n",
       "  'news latest',\n",
       "  'latest war',\n",
       "  'todays breaking',\n",
       "  'breaking news']]"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_v6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "bb3aa030-f1d3-45f2-abdc-490d44904f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_feature_names = set(LSAPipeline.tfidf_vectorizer.get_feature_names_out())\n",
    "preprocessed_documents = LSAPipeline.import_documents_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "755e5cd1-aeed-4605-9ea3-621eed4150f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of ngrams into a list of lists of words\n",
    "texts_v6 = [[word for word in doc.lower().split() if (\n",
    "    word in tfidf_feature_names)] + tokenize_ngrams(doc, 2) for doc in preprocessed_documents]\n",
    "\n",
    "# Create a Gensim dictionary\n",
    "dictionary_v6 = Dictionary(texts_v6)\n",
    "\n",
    "# Convert the dictionary and the corpus\n",
    "corpus_v6 = [dictionary_v6.doc2bow(text) for text in texts_v6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "9fa15b20-5928-45e5-9218-e14b5260f0a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Score: 0.8521311651349903\n"
     ]
    }
   ],
   "source": [
    "# Calculate the coherence score using Gensim\n",
    "coherence_model = CoherenceModel(topics=topics_v6,\n",
    "                                 texts=texts_v6,\n",
    "                                 dictionary=dictionary_v6,\n",
    "                                 corpus=corpus_v6,\n",
    "                                 coherence='c_v')\n",
    "coherence_score = coherence_model.get_coherence()\n",
    "\n",
    "print('Coherence Score:', coherence_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fc40cc-f66c-47b9-911e-2521cf420772",
   "metadata": {},
   "source": [
    "With no not alphanumeric sentence members removal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "a2429f6b-0d6b-49a3-9aed-6ea510b2cd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnglishPreprocessor:\n",
    "    def preprocess_documents(self, documents_list: list, return_strings: bool = True):\n",
    "        return [self.preprocess(doc, return_string=return_strings) for doc in documents_list]\n",
    "    \n",
    "    # Function to preprocess documents\n",
    "    def preprocess(self, document, return_string: bool = True):\n",
    "        # Tokenize\n",
    "        document = self.remove_links_content(document)\n",
    "        document = self.remove_emails(document)\n",
    "        document = self.remove_multiple_space(document)\n",
    "        document = self.remove_hashtags(document)\n",
    "        document = self.remove_punctuation(document)\n",
    "        document = self.remove_multiple_space(document)\n",
    "        \n",
    "        words = word_tokenize(document.lower())\n",
    "        # Remove stopwords and punctuations\n",
    "        filtered_words = [word for word in words if not word in stop_words]\n",
    "        # Lemmatize\n",
    "        lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
    "        if return_string:\n",
    "            return ' '.join(lemmatized_words)\n",
    "        else:\n",
    "            return lemmatized_words\n",
    "    \n",
    "    def remove_links_content(self, text):\n",
    "        text = re.sub(r\"http\\S+\", \"\", text)\n",
    "        return text\n",
    "    \n",
    "    def remove_emails(self, text):\n",
    "        return re.sub('\\S+@\\S*\\s?', '', text)\n",
    "    \n",
    "    def remove_punctuation(self, text):\n",
    "        \"\"\"https://stackoverflow.com/a/37221663\"\"\"\n",
    "        table = str.maketrans({key: None for key in string.punctuation})\n",
    "        return text.translate(table)\n",
    "    \n",
    "    def remove_multiple_space(self, text):\n",
    "        return re.sub(\"\\s\\s+\", \" \", text)\n",
    "\n",
    "    def remove_hashtags(self, text):\n",
    "        old_text = text + '\\n'\n",
    "        new_text = text\n",
    "        while len(new_text) < len(old_text):\n",
    "            old_text = new_text\n",
    "            new_text = re.sub('(?<=[\\s\\n])#\\S+\\s*$', '', new_text)\n",
    "        return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "b80564d7-9a9d-4f8f-bbf1-5b3974f482cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "LSAPipeline = LSAPipelineEnglish(all_documents_19082308, tf_idf_max_df=0.9, tf_idf_min_df=4,\n",
    "                                 lsa_components=5, svd_n_iter=5,\n",
    "                                 n_top_words=10, ngram_range=(1,2),\n",
    "                                 random_state=47)\n",
    "topics_v6 = LSAPipeline.run_topics_detection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "34d47231-2f4d-42d6-ac84-698e52dc3074",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ukraine',\n",
       "  'russia',\n",
       "  'russian',\n",
       "  'war',\n",
       "  'putin',\n",
       "  'ukrainian',\n",
       "  'day',\n",
       "  'amp',\n",
       "  'people',\n",
       "  'state'],\n",
       " ['possible russia',\n",
       "  'slavaukraine russia',\n",
       "  'russian barbarism',\n",
       "  'barbarism limit',\n",
       "  'limit peace',\n",
       "  'peace possible',\n",
       "  'russia pariah',\n",
       "  'defeated ukrainian',\n",
       "  'pariah state',\n",
       "  'putin russian'],\n",
       " ['passive',\n",
       "  'awesome',\n",
       "  'hire',\n",
       "  'affiliate',\n",
       "  'long term',\n",
       "  'income',\n",
       "  'term passive',\n",
       "  'autopilot',\n",
       "  'developer',\n",
       "  'term'],\n",
       " ['putin',\n",
       "  'russia',\n",
       "  'fuck',\n",
       "  'trump',\n",
       "  'russia putin',\n",
       "  'karilake seanhannity',\n",
       "  'donaldjtrumpjr repmtg',\n",
       "  'kimkbaltimore karilake',\n",
       "  'kimkbaltimore',\n",
       "  'tuckercarlson tedcruz'],\n",
       " ['russian',\n",
       "  'putin',\n",
       "  'daughter',\n",
       "  'fuck',\n",
       "  'dugin',\n",
       "  'ukrainian',\n",
       "  'car',\n",
       "  'killed',\n",
       "  'fuck putin',\n",
       "  'video']]"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_v6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "40f19cce-0d92-4a3d-a294-98b09fd98369",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_feature_names = set(LSAPipeline.tfidf_vectorizer.get_feature_names_out())\n",
    "preprocessed_documents = LSAPipeline.import_documents_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "350d3b84-fa3e-49b1-aef3-9171053ef9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of ngrams into a list of lists of words\n",
    "texts_v6 = [[word for word in doc.lower().split() if (\n",
    "    word in tfidf_feature_names)] + tokenize_ngrams(doc, 2) for doc in preprocessed_documents]\n",
    "\n",
    "# Create a Gensim dictionary\n",
    "dictionary_v6 = Dictionary(texts_v6)\n",
    "\n",
    "# Convert the dictionary and the corpus\n",
    "corpus_v6 = [dictionary_v6.doc2bow(text) for text in texts_v6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "50e6c186-8393-409b-b542-12e730d430f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Score: 0.6971141105763069\n"
     ]
    }
   ],
   "source": [
    "# Calculate the coherence score using Gensim\n",
    "coherence_model = CoherenceModel(topics=topics_v6,\n",
    "                                 texts=texts_v6,\n",
    "                                 dictionary=dictionary_v6,\n",
    "                                 corpus=corpus_v6,\n",
    "                                 coherence='c_v')\n",
    "coherence_score = coherence_model.get_coherence()\n",
    "\n",
    "print('Coherence Score:', coherence_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c8be4e-1974-4ec2-87e2-5312ff5b490b",
   "metadata": {},
   "source": [
    "With no stopwords removal, lemmatization or not alphanumeric sentence members removal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "151bb206-a3f7-4be1-9112-3e4a2e556560",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnglishPreprocessor:\n",
    "    def preprocess_documents(self, documents_list: list, return_strings: bool = True):\n",
    "        return [self.preprocess(doc, return_string=return_strings) for doc in documents_list]\n",
    "    \n",
    "    # Function to preprocess documents\n",
    "    def preprocess(self, document, return_string: bool = True):\n",
    "        # Tokenize\n",
    "        document = self.remove_links_content(document)\n",
    "        document = self.remove_emails(document)\n",
    "        document = self.remove_multiple_space(document)\n",
    "        document = self.remove_hashtags(document)\n",
    "        document = self.remove_punctuation(document)\n",
    "        document = self.remove_multiple_space(document)\n",
    "        \n",
    "        words = word_tokenize(document.lower())\n",
    "        if return_string:\n",
    "            return ' '.join(words)\n",
    "        else:\n",
    "            return words\n",
    "    \n",
    "    def remove_links_content(self, text):\n",
    "        text = re.sub(r\"http\\S+\", \"\", text)\n",
    "        return text\n",
    "    \n",
    "    def remove_emails(self, text):\n",
    "        return re.sub('\\S+@\\S*\\s?', '', text)\n",
    "    \n",
    "    def remove_punctuation(self, text):\n",
    "        \"\"\"https://stackoverflow.com/a/37221663\"\"\"\n",
    "        table = str.maketrans({key: None for key in string.punctuation})\n",
    "        return text.translate(table)\n",
    "    \n",
    "    def remove_multiple_space(self, text):\n",
    "        return re.sub(\"\\s\\s+\", \" \", text)\n",
    "\n",
    "    def remove_hashtags(self, text):\n",
    "        old_text = text + '\\n'\n",
    "        new_text = text\n",
    "        while len(new_text) < len(old_text):\n",
    "            old_text = new_text\n",
    "            new_text = re.sub('(?<=[\\s\\n])#\\S+\\s*$', '', new_text)\n",
    "        return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "b6f6ec7a-6bfb-44d1-ae15-f9828db7ee7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "LSAPipeline = LSAPipelineEnglish(all_documents_19082308, tf_idf_max_df=0.9, tf_idf_min_df=4,\n",
    "                                 lsa_components=5, svd_n_iter=5,\n",
    "                                 n_top_words=10, ngram_range=(1,2),\n",
    "                                 random_state=47)\n",
    "topics_v6 = LSAPipeline.run_topics_detection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "61847f42-701e-4006-912b-36116f6a0443",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ukraine',\n",
       "  'russia',\n",
       "  'war',\n",
       "  'russian',\n",
       "  'putin',\n",
       "  'ukrainian',\n",
       "  'russians',\n",
       "  'amp',\n",
       "  'day',\n",
       "  'people'],\n",
       " ['russians',\n",
       "  'possible russia',\n",
       "  'putin russians',\n",
       "  'defeated ukrainians',\n",
       "  'peace possible',\n",
       "  'barbarism limits',\n",
       "  'russians barbarism',\n",
       "  'slavaukraine russia',\n",
       "  'limits peace',\n",
       "  'russia pariah'],\n",
       " ['passive',\n",
       "  'hire',\n",
       "  'awesome',\n",
       "  'affiliate',\n",
       "  'term',\n",
       "  'long term',\n",
       "  'income',\n",
       "  'term passive',\n",
       "  'autopilot',\n",
       "  'developer'],\n",
       " ['putin',\n",
       "  'russia putin',\n",
       "  'kimkbaltimore karilake',\n",
       "  'seanhannity tuckercarlson',\n",
       "  'donaldjtrumpjr repmtg',\n",
       "  'tuckercarlson tedcruz',\n",
       "  'karilake seanhannity',\n",
       "  'kimkbaltimore',\n",
       "  'seanhannity',\n",
       "  'trump'],\n",
       " ['standwithukraine',\n",
       "  'playing',\n",
       "  'standwithukraine playing',\n",
       "  'video',\n",
       "  'russian',\n",
       "  'putin',\n",
       "  'ukrainian',\n",
       "  'fuck',\n",
       "  'daughter',\n",
       "  'dugin']]"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_v6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "af1ca00f-38d3-4da3-a56e-e679ff3c5f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_feature_names = set(LSAPipeline.tfidf_vectorizer.get_feature_names_out())\n",
    "preprocessed_documents = LSAPipeline.import_documents_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "25f0f3da-5214-44b2-b93c-9b31c6432222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of ngrams into a list of lists of words\n",
    "texts_v6 = [[word for word in doc.lower().split() if (\n",
    "    word in tfidf_feature_names)] + tokenize_ngrams(doc, 2) for doc in preprocessed_documents]\n",
    "\n",
    "# Create a Gensim dictionary\n",
    "dictionary_v6 = Dictionary(texts_v6)\n",
    "\n",
    "# Convert the dictionary and the corpus\n",
    "corpus_v6 = [dictionary_v6.doc2bow(text) for text in texts_v6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "531fb143-acb4-4c15-a848-31c93deb4728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Score: 0.6345531265408078\n"
     ]
    }
   ],
   "source": [
    "# Calculate the coherence score using Gensim\n",
    "coherence_model = CoherenceModel(topics=topics_v6,\n",
    "                                 texts=texts_v6,\n",
    "                                 dictionary=dictionary_v6,\n",
    "                                 corpus=corpus_v6,\n",
    "                                 coherence='c_v')\n",
    "coherence_score = coherence_model.get_coherence()\n",
    "\n",
    "print('Coherence Score:', coherence_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5916c090-5c1e-4a89-8ca0-561eb8367dcc",
   "metadata": {},
   "source": [
    "With no stopwords removal and lemmatization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "2a0bbc16-eed2-4d37-9d7e-6fbfc1c5e601",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnglishPreprocessor:\n",
    "    def preprocess_documents(self, documents_list: list, return_strings: bool = True):\n",
    "        return [self.preprocess(doc, return_string=return_strings) for doc in documents_list]\n",
    "    \n",
    "    # Function to preprocess documents\n",
    "    def preprocess(self, document, return_string: bool = True):\n",
    "        # Tokenize\n",
    "        document = self.remove_links_content(document)\n",
    "        document = self.remove_emails(document)\n",
    "        document = self.remove_multiple_space(document)\n",
    "        document = self.remove_hashtags(document)\n",
    "        document = self.remove_punctuation(document)\n",
    "        document = self.remove_multiple_space(document)\n",
    "        \n",
    "        words = word_tokenize(document.lower())\n",
    "        # Remove stopwords and punctuations\n",
    "        filtered_words = [word for word in words if word.isalnum()]\n",
    "        if return_string:\n",
    "            return ' '.join(filtered_words)\n",
    "        else:\n",
    "            return filtered_words\n",
    "    \n",
    "    def remove_links_content(self, text):\n",
    "        text = re.sub(r\"http\\S+\", \"\", text)\n",
    "        return text\n",
    "    \n",
    "    def remove_emails(self, text):\n",
    "        return re.sub('\\S+@\\S*\\s?', '', text)\n",
    "    \n",
    "    def remove_punctuation(self, text):\n",
    "        \"\"\"https://stackoverflow.com/a/37221663\"\"\"\n",
    "        table = str.maketrans({key: None for key in string.punctuation})\n",
    "        return text.translate(table)\n",
    "    \n",
    "    def remove_multiple_space(self, text):\n",
    "        return re.sub(\"\\s\\s+\", \" \", text)\n",
    "\n",
    "    def remove_hashtags(self, text):\n",
    "        old_text = text + '\\n'\n",
    "        new_text = text\n",
    "        while len(new_text) < len(old_text):\n",
    "            old_text = new_text\n",
    "            new_text = re.sub('(?<=[\\s\\n])#\\S+\\s*$', '', new_text)\n",
    "        return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "b07f0908-c3f2-48a1-82c8-f148e242fa01",
   "metadata": {},
   "outputs": [],
   "source": [
    "LSAPipeline = LSAPipelineEnglish(all_documents_19082308, tf_idf_max_df=0.9, tf_idf_min_df=4,\n",
    "                                 lsa_components=5, svd_n_iter=5,\n",
    "                                 n_top_words=10, ngram_range=(1,2),\n",
    "                                 random_state=47)\n",
    "topics_v6 = LSAPipeline.run_topics_detection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "748ebcb6-da1e-44ef-8ad6-9d6daf972e2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ukraine',\n",
       "  'russia',\n",
       "  'war',\n",
       "  'russian',\n",
       "  'putin',\n",
       "  'russians',\n",
       "  'ukrainian',\n",
       "  'amp',\n",
       "  'day',\n",
       "  'people'],\n",
       " ['passive',\n",
       "  'awesome',\n",
       "  'hire',\n",
       "  'term',\n",
       "  'affiliate',\n",
       "  'income',\n",
       "  'long term',\n",
       "  'term passive',\n",
       "  'autopilot',\n",
       "  'website'],\n",
       " ['russians',\n",
       "  'possible russia',\n",
       "  'limits peace',\n",
       "  'russians barbarism',\n",
       "  'barbarism limits',\n",
       "  'defeated ukrainians',\n",
       "  'peace possible',\n",
       "  'slavaukraine russia',\n",
       "  'putin russians',\n",
       "  'russia pariah'],\n",
       " ['putin',\n",
       "  'trump',\n",
       "  'russia putin',\n",
       "  'karilake seanhannity',\n",
       "  'donaldjtrumpjr repmtg',\n",
       "  'tuckercarlson tedcruz',\n",
       "  'seanhannity tuckercarlson',\n",
       "  'kimkbaltimore karilake',\n",
       "  'kimkbaltimore',\n",
       "  'seanhannity'],\n",
       " ['standwithukraine',\n",
       "  'playing',\n",
       "  'standwithukraine playing',\n",
       "  'video',\n",
       "  'russian',\n",
       "  'putin',\n",
       "  'fuck',\n",
       "  'daughter',\n",
       "  'dugin',\n",
       "  'ukrainian']]"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_v6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "40c0490c-ba97-41f6-9972-d9933616b1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_feature_names = set(LSAPipeline.tfidf_vectorizer.get_feature_names_out())\n",
    "preprocessed_documents = LSAPipeline.import_documents_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "507b9e9a-4883-4a57-8023-5694800f5939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of ngrams into a list of lists of words\n",
    "texts_v6 = [[word for word in doc.lower().split() if (\n",
    "    word in tfidf_feature_names)] + tokenize_ngrams(doc, 2) for doc in preprocessed_documents]\n",
    "\n",
    "# Create a Gensim dictionary\n",
    "dictionary_v6 = Dictionary(texts_v6)\n",
    "\n",
    "# Convert the dictionary and the corpus\n",
    "corpus_v6 = [dictionary_v6.doc2bow(text) for text in texts_v6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "30f981c2-9b91-49d4-9c28-9666fa85bd7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Score: 0.6345147773447113\n"
     ]
    }
   ],
   "source": [
    "# Calculate the coherence score using Gensim\n",
    "coherence_model = CoherenceModel(topics=topics_v6,\n",
    "                                 texts=texts_v6,\n",
    "                                 dictionary=dictionary_v6,\n",
    "                                 corpus=corpus_v6,\n",
    "                                 coherence='c_v')\n",
    "coherence_score = coherence_model.get_coherence()\n",
    "\n",
    "print('Coherence Score:', coherence_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3700071-35c0-4af9-8b8a-87a0981b56c3",
   "metadata": {},
   "source": [
    "With no lemmatization and no not alphanumeric sentence members removal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "5ab55126-54aa-46f5-bdbf-4134dd85d9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnglishPreprocessor:\n",
    "    def preprocess_documents(self, documents_list: list, return_strings: bool = True):\n",
    "        return [self.preprocess(doc, return_string=return_strings) for doc in documents_list]\n",
    "    \n",
    "    # Function to preprocess documents\n",
    "    def preprocess(self, document, return_string: bool = True):\n",
    "        # Tokenize\n",
    "        document = self.remove_links_content(document)\n",
    "        document = self.remove_emails(document)\n",
    "        document = self.remove_multiple_space(document)\n",
    "        document = self.remove_hashtags(document)\n",
    "        document = self.remove_punctuation(document)\n",
    "        document = self.remove_multiple_space(document)\n",
    "        \n",
    "        words = word_tokenize(document.lower())\n",
    "        # Remove stopwords and punctuations\n",
    "        filtered_words = [word for word in words if not word in stop_words]\n",
    "        if return_string:\n",
    "            return ' '.join(filtered_words)\n",
    "        else:\n",
    "            return filtered_words\n",
    "    \n",
    "    def remove_links_content(self, text):\n",
    "        text = re.sub(r\"http\\S+\", \"\", text)\n",
    "        return text\n",
    "    \n",
    "    def remove_emails(self, text):\n",
    "        return re.sub('\\S+@\\S*\\s?', '', text)\n",
    "    \n",
    "    def remove_punctuation(self, text):\n",
    "        \"\"\"https://stackoverflow.com/a/37221663\"\"\"\n",
    "        table = str.maketrans({key: None for key in string.punctuation})\n",
    "        return text.translate(table)\n",
    "    \n",
    "    def remove_multiple_space(self, text):\n",
    "        return re.sub(\"\\s\\s+\", \" \", text)\n",
    "\n",
    "    def remove_hashtags(self, text):\n",
    "        old_text = text + '\\n'\n",
    "        new_text = text\n",
    "        while len(new_text) < len(old_text):\n",
    "            old_text = new_text\n",
    "            new_text = re.sub('(?<=[\\s\\n])#\\S+\\s*$', '', new_text)\n",
    "        return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "6b2badc5-5c74-46d3-84d0-d31b68f24be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "LSAPipeline = LSAPipelineEnglish(all_documents_19082308, tf_idf_max_df=0.9, tf_idf_min_df=4,\n",
    "                                 lsa_components=5, svd_n_iter=5,\n",
    "                                 n_top_words=10, ngram_range=(1,2),\n",
    "                                 random_state=47)\n",
    "topics_v6 = LSAPipeline.run_topics_detection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "97bba1b2-bb56-4cca-8ebc-476f61e20b64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ukraine',\n",
       "  'russia',\n",
       "  'war',\n",
       "  'russian',\n",
       "  'putin',\n",
       "  'russians',\n",
       "  'ukrainian',\n",
       "  'amp',\n",
       "  'day',\n",
       "  'people'],\n",
       " ['russians',\n",
       "  'putin russians',\n",
       "  'possible russia',\n",
       "  'peace possible',\n",
       "  'russians barbarism',\n",
       "  'limits peace',\n",
       "  'defeated ukrainians',\n",
       "  'barbarism limits',\n",
       "  'slavaukraine russia',\n",
       "  'russia pariah'],\n",
       " ['passive',\n",
       "  'hire',\n",
       "  'awesome',\n",
       "  'affiliate',\n",
       "  'term',\n",
       "  'long term',\n",
       "  'income',\n",
       "  'term passive',\n",
       "  'autopilot',\n",
       "  'developer'],\n",
       " ['putin',\n",
       "  'trump',\n",
       "  'russia putin',\n",
       "  'kimkbaltimore karilake',\n",
       "  'seanhannity tuckercarlson',\n",
       "  'donaldjtrumpjr repmtg',\n",
       "  'karilake seanhannity',\n",
       "  'tuckercarlson tedcruz',\n",
       "  'kimkbaltimore',\n",
       "  'seanhannity'],\n",
       " ['ukraine',\n",
       "  'war',\n",
       "  'news',\n",
       "  'russia ukraine',\n",
       "  'news latest',\n",
       "  'latest war',\n",
       "  'todays breaking',\n",
       "  'war russia',\n",
       "  'breaking news',\n",
       "  'todays']]"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_v6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "2e5a9ca9-2da3-44e8-9110-da39896e7b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_feature_names = set(LSAPipeline.tfidf_vectorizer.get_feature_names_out())\n",
    "preprocessed_documents = LSAPipeline.import_documents_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "942490c6-c13f-49ff-a9a4-52fa07383c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of ngrams into a list of lists of words\n",
    "texts_v6 = [[word for word in doc.lower().split() if (\n",
    "    word in tfidf_feature_names)] + tokenize_ngrams(doc, 2) for doc in preprocessed_documents]\n",
    "\n",
    "# Create a Gensim dictionary\n",
    "dictionary_v6 = Dictionary(texts_v6)\n",
    "\n",
    "# Convert the dictionary and the corpus\n",
    "corpus_v6 = [dictionary_v6.doc2bow(text) for text in texts_v6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "3fb70720-6d9d-4a80-91fc-c55c0a0a4ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Score: 0.8293871089413314\n"
     ]
    }
   ],
   "source": [
    "# Calculate the coherence score using Gensim\n",
    "coherence_model = CoherenceModel(topics=topics_v6,\n",
    "                                 texts=texts_v6,\n",
    "                                 dictionary=dictionary_v6,\n",
    "                                 corpus=corpus_v6,\n",
    "                                 coherence='c_v')\n",
    "coherence_score = coherence_model.get_coherence()\n",
    "\n",
    "print('Coherence Score:', coherence_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a571b36-83e7-4619-bc29-bf1169db3c5d",
   "metadata": {},
   "source": [
    "## 1- and 2-grams with no lemmatization: optimal topics number\n",
    "\n",
    "Model that uses both 1- and 2-grams with no lemmatization for the different topics numbers for the data for September 2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a3a777-112a-44e6-a889-ec40b4e972c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnglishPreprocessor:\n",
    "    def preprocess_documents(self, documents_list: list, return_strings: bool = True):\n",
    "        return [self.preprocess(doc, return_string=return_strings) for doc in documents_list]\n",
    "    \n",
    "    # Function to preprocess documents\n",
    "    def preprocess(self, document, return_string: bool = True):\n",
    "        # Tokenize\n",
    "        document = self.remove_links_content(document)\n",
    "        document = self.remove_emails(document)\n",
    "        document = self.remove_multiple_space(document)\n",
    "        document = self.remove_hashtags(document)\n",
    "        document = self.remove_punctuation(document)\n",
    "        document = self.remove_multiple_space(document)\n",
    "        \n",
    "        words = word_tokenize(document.lower())\n",
    "        # Remove stopwords and punctuations\n",
    "        filtered_words = [word for word in words if word.isalnum() and not word in stop_words]\n",
    "        if return_string:\n",
    "            return ' '.join(filtered_words)\n",
    "        else:\n",
    "            return filtered_words\n",
    "    \n",
    "    def remove_links_content(self, text):\n",
    "        text = re.sub(r\"http\\S+\", \"\", text)\n",
    "        return text\n",
    "    \n",
    "    def remove_emails(self, text):\n",
    "        return re.sub('\\S+@\\S*\\s?', '', text)\n",
    "    \n",
    "    def remove_punctuation(self, text):\n",
    "        \"\"\"https://stackoverflow.com/a/37221663\"\"\"\n",
    "        table = str.maketrans({key: None for key in string.punctuation})\n",
    "        return text.translate(table)\n",
    "    \n",
    "    def remove_multiple_space(self, text):\n",
    "        return re.sub(\"\\s\\s+\", \" \", text)\n",
    "\n",
    "    def remove_hashtags(self, text):\n",
    "        old_text = text + '\\n'\n",
    "        new_text = text\n",
    "        while len(new_text) < len(old_text):\n",
    "            old_text = new_text\n",
    "            new_text = re.sub('(?<=[\\s\\n])#\\S+\\s*$', '', new_text)\n",
    "        return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfa80d9-e885-435d-9b85-f41922efd804",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_numbers_12grams = list(range(2,4))\n",
    "coherences_12grams = []\n",
    "predicted_topics_counts_12grams = []\n",
    "for topic_number in topic_numbers_12grams:\n",
    "    LSAPipeline = LSAPipelineEnglish(all_documents_19082308, tf_idf_max_df=0.9, tf_idf_min_df=4,\n",
    "                                 lsa_components=topic_number, svd_n_iter=5,\n",
    "                                 n_top_words=10, ngram_range=(1,2),\n",
    "                                 random_state=47)\n",
    "    topics_12grams = LSAPipeline.run_topics_detection()\n",
    "    tfidf_feature_names = set(LSAPipeline.tfidf_vectorizer.get_feature_names_out())\n",
    "    # Calculate the coherence score using Gensim\n",
    "    coherence_model = CoherenceModel(topics=topics_12grams,\n",
    "                                     texts=texts_v4,\n",
    "                                     dictionary=dictionary_v4,\n",
    "                                     corpus=corpus_v4,\n",
    "                                     coherence='c_v')\n",
    "    coherence_score = coherence_model.get_coherence()\n",
    "    predicted_topics = LSAPipeline.transform_documents(LSAPipeline.import_documents_list)\n",
    "    predicted_topics_count = [predicted_topics.count(x) for x in np.sort(np.unique(predicted_topics))]\n",
    "    \n",
    "    print(f'--Topic number {topic_number} coherence score: {coherence_score}')\n",
    "    coherences_12grams.append(coherence_score)\n",
    "    predicted_topics_counts_12grams.append(predicted_topics_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b796ba-81ad-4e42-9179-fa092b34e75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('coherences_12grams_no_lemmatization.json','w') as json_file:\n",
    "    json.dump({'coherences': coherences_12grams,\n",
    "               'predicted_topics': predicted_topics_counts_12grams}, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db381247-3d4f-4900-9246-ee1f9269e8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(coherences_12grams, topic_numbers_12grams)\n",
    "plt.title('Coherence for different n_topics | 12grams model, no lemmatization')\n",
    "plt.xlabel('Coherence (c_v) score')\n",
    "plt.ylabel('Topics number')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af955b0e-5d29-4706-9656-67c28dade4b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68890b83-1e86-4fa2-9c1f-1b72c4277f6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ca3515-0e25-4db0-ba25-a47cd9cddd76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
